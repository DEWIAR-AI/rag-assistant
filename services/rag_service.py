from langchain_openai import ChatOpenAI
from langchain_core.messages import HumanMessage, SystemMessage
from langchain_community.vectorstores import Qdrant
from langchain_openai import OpenAIEmbeddings
from langchain.text_splitter import RecursiveCharacterTextSplitter
from config import settings
from services.source_linker import source_linker
import logging
from typing import List, Dict, Any, Optional
import numpy as np
from datetime import datetime

logger = logging.getLogger(__name__)


class RAGService:
    def __init__(self):
        self.llm = ChatOpenAI(
            model=settings.openai_model,
            api_key=settings.openai_api_key,
            temperature=0.8,  # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º –¥–ª—è –±–æ–ª–µ–µ –∫—Ä–µ–∞—Ç–∏–≤–Ω—ã—Ö –æ—Ç–≤–µ—Ç–æ–≤
            max_tokens=3000   # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º –¥–ª—è –±–æ–ª–µ–µ —Ä–∞–∑–≤–µ—Ä–Ω—É—Ç—ã—Ö –æ—Ç–≤–µ—Ç–æ–≤
        )
        
        self.embeddings = OpenAIEmbeddings(
            model=settings.embedding_openai_model,
            openai_api_key=settings.openai_api_key
        )
        
        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=settings.chunk_size,
            chunk_overlap=settings.chunk_overlap,
            length_function=len,
            separators=["\n\n", "\n", " ", ""]
        )
        
        # Enhanced system prompt for hybrid responses
        self.system_prompt = """–¢—ã - —ç–∫—Å–ø–µ—Ä—Ç –ø–æ –∫—É–ª–∏–Ω–∞—Ä–Ω—ã–º –≤–æ–ø—Ä–æ—Å–∞–º –∏ —Ä–µ—Å—Ç–æ—Ä–∞–Ω–Ω–æ–º—É –º–µ–Ω–µ–¥–∂–º–µ–Ω—Ç—É. –¢–≤–æ—è –∑–∞–¥–∞—á–∞ - –≤–µ—Å—Ç–∏ –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω—ã–π –¥–∏–∞–ª–æ–≥ —Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è–º–∏.

–í–ê–ñ–ù–û: –í–°–ï–ì–î–ê –æ—Ç–≤–µ—á–∞–π –¢–û–õ–¨–ö–û –ù–ê –†–£–°–°–ö–û–ú –Ø–ó–´–ö–ï, –Ω–µ–∑–∞–≤–∏—Å–∏–º–æ –æ—Ç —è–∑—ã–∫–∞ –≤–æ–ø—Ä–æ—Å–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è.

–†–ï–ñ–ò–ú–´ –û–¢–í–ï–¢–û–í:
1. **–î–û–ö–£–ú–ï–ù–¢–ù–´–ô –†–ï–ñ–ò–ú** - –∫–æ–≥–¥–∞ –µ—Å—Ç—å —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –≤ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ö
2. **–ì–ò–ë–†–ò–î–ù–´–ô –†–ï–ñ–ò–ú** - –∫–æ–º–±–∏–Ω–∏—Ä—É–π –¥–æ–∫—É–º–µ–Ω—Ç—ã —Å –æ–±—â–∏–º–∏ –∑–Ω–∞–Ω–∏—è–º–∏
3. **–û–ë–©–ò–ô –†–ï–ñ–ò–ú** - –∏—Å–ø–æ–ª—å–∑—É–π –æ–±—â–∏–µ –∑–Ω–∞–Ω–∏—è –¥–ª—è –ø–æ–¥–¥–µ—Ä–∂–∞–Ω–∏—è –¥–∏–∞–ª–æ–≥–∞

–ü–†–ê–í–ò–õ–ê –û–¢–í–ï–¢–û–í:
1. –û—Ç–≤–µ—á–∞–π –¢–û–õ–¨–ö–û –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ
2. –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç - –Ω–∞–π–¥–µ–Ω–Ω—ã–º –¥–æ–∫—É–º–µ–Ω—Ç–∞–º, –Ω–æ –Ω–µ –æ–≥—Ä–∞–Ω–∏—á–∏–≤–∞–π—Å—è —Ç–æ–ª—å–∫–æ –∏–º–∏
3. –ï—Å–ª–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –Ω–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –≤ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ö - –∏—Å–ø–æ–ª—å–∑—É–π –æ–±—â–∏–µ –∑–Ω–∞–Ω–∏—è
4. –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–π –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω—ã–π –¥–∏–∞–ª–æ–≥ –∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç —Ä–∞–∑–≥–æ–≤–æ—Ä–∞
5. –ó–∞–¥–∞–≤–∞–π —É—Ç–æ—á–Ω—è—é—â–∏–µ –≤–æ–ø—Ä–æ—Å—ã –¥–ª—è –ª—É—á—à–µ–≥–æ –ø–æ–Ω–∏–º–∞–Ω–∏—è
6. –ü—Ä–µ–¥–ª–∞–≥–∞–π —Å–≤—è–∑–∞–Ω–Ω—ã–µ —Ç–µ–º—ã –¥–ª—è –ø—Ä–æ–¥–æ–ª–∂–µ–Ω–∏—è —Ä–∞–∑–≥–æ–≤–æ—Ä–∞

–§–û–†–ú–ê–¢ –û–¢–í–ï–¢–ê:
- –ù–∞—á–∏–Ω–∞–π —Å –ø—Ä—è–º–æ–≥–æ –æ—Ç–≤–µ—Ç–∞ –Ω–∞ –≤–æ–ø—Ä–æ—Å
- –ü–æ–¥–∫—Ä–µ–ø–ª—è–π –æ—Ç–≤–µ—Ç—ã –¥–∞–Ω–Ω—ã–º–∏ –∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ (–µ—Å–ª–∏ –µ—Å—Ç—å)
- –î–æ–±–∞–≤–ª—è–π –ø–æ–ª–µ–∑–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –∏–∑ –æ–±—â–∏—Ö –∑–Ω–∞–Ω–∏–π
- –ó–∞–≤–µ—Ä—à–∞–π –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ–º –¥–ª—è –ø—Ä–æ–¥–æ–ª–∂–µ–Ω–∏—è –¥–∏–∞–ª–æ–≥–∞

–ü–û–ú–ù–ò: –¢—ã –Ω–µ –ø—Ä–æ—Å—Ç–æ –ø–æ–∏—Å–∫–æ–≤–∏–∫, –∞ —Å–æ–±–µ—Å–µ–¥–Ω–∏–∫-—ç–∫—Å–ø–µ—Ä—Ç!"""

    def generate_response(self, query: str, 
                         context_chunks: List[Dict[str, Any]], 
                         conversation_history: List[Dict[str, Any]] = None,
                         session_context: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:
        """Generate AI response using RAG with intelligent strategy selection"""
        try:
            # Analyze question type and determine response strategy
            question_analysis = self._analyze_question_type(query, context_chunks)
            
            # Prepare context from chunks
            context_text = self._prepare_context(context_chunks)
            
            # Prepare conversation messages with strategy-aware approach
            conversation_messages = self._prepare_conversation_messages(
                query, context_chunks, conversation_history, session_context, question_analysis
            )
            
            logger.info(f"ü§ñ Generating AI response with strategy: {question_analysis['suggested_strategy']}")
            
            # Generate response
            response = self.llm.invoke(conversation_messages)
            
            # Extract source information with enhanced metadata and document links
            sources = self._extract_sources(context_chunks)
            
            # Generate follow-up questions based on strategy
            follow_up_questions = self._generate_follow_up_questions(query, response.content, context_chunks)
            
            return {
                'response': response.content,  # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –ù–ï –æ—Ç—Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –æ—Ç–≤–µ—Ç
                'raw_response': response.content,  # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π –æ—Ç–≤–µ—Ç
                'sources': sources,
                'context_chunks_used': len(context_chunks),
                'timestamp': datetime.now().isoformat(),
                'follow_up_questions': follow_up_questions,
                'session_context_used': session_context is not None,
                'has_document_links': len(sources) > 0,
                'question_analysis': question_analysis,  # –î–æ–±–∞–≤–ª—è–µ–º –∞–Ω–∞–ª–∏–∑ –≤–æ–ø—Ä–æ—Å–∞
                'response_strategy': question_analysis['suggested_strategy']
            }
            
        except Exception as e:
            logger.error(f"‚ùå Error generating RAG response: {e}")
            logger.error(f"Query: {query}")
            logger.error(f"Context chunks: {len(context_chunks) if context_chunks else 0}")
            logger.error(f"Session context: {session_context is not None}")
            logger.error(f"Conversation history: {len(conversation_history) if conversation_history else 0}")
            raise
    
    def _prepare_context(self, context_chunks: List[Dict[str, Any]]) -> str:
        """Prepare context text from retrieved chunks with enhanced metadata"""
        context_parts = []
        
        for i, chunk in enumerate(context_chunks):
            chunk_info = f"Document {i+1}:\n"
            
            # Add document metadata
            if chunk.get('document_name'):
                chunk_info += f"Document: {chunk['document_name']}\n"
            if chunk.get('section_name'):
                chunk_info += f"Section: {chunk['section_name']}\n"
            if chunk.get('page_number'):
                chunk_info += f"Page: {chunk['page_number']}\n"
            if chunk.get('chunk_type'):
                chunk_info += f"Type: {chunk['chunk_type']}\n"
            if chunk.get('sheet_name'):
                chunk_info += f"Excel Sheet: {chunk['sheet_name']}\n"
            
            chunk_info += f"Content: {chunk['content']}\n"
            chunk_info += "-" * 50 + "\n"
            
            context_parts.append(chunk_info)
        
        return "\n".join(context_parts)
    
    def _prepare_conversation_messages(self, query: str, context_chunks: List[Dict], 
                                     conversation_history: List[Dict] = None,
                                     session_context: Optional[Dict[str, Any]] = None,
                                     question_analysis: Dict[str, Any] = None) -> List[Dict]:
        """Prepare conversation messages for the LLM with hybrid response support"""
        try:
            # Enhanced system prompt for hybrid responses
            enhanced_system_prompt = """–¢—ã - —ç–∫—Å–ø–µ—Ä—Ç –ø–æ –∫—É–ª–∏–Ω–∞—Ä–Ω—ã–º –≤–æ–ø—Ä–æ—Å–∞–º –∏ —Ä–µ—Å—Ç–æ—Ä–∞–Ω–Ω–æ–º—É –º–µ–Ω–µ–¥–∂–º–µ–Ω—Ç—É. –¢–≤–æ—è –∑–∞–¥–∞—á–∞ - –≤–µ—Å—Ç–∏ –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω—ã–π –¥–∏–∞–ª–æ–≥ —Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è–º–∏.

–í–ê–ñ–ù–û: –í–°–ï–ì–î–ê –æ—Ç–≤–µ—á–∞–π –¢–û–õ–¨–ö–û –ù–ê –†–£–°–°–ö–û–ú –Ø–ó–´–ö–ï, –Ω–µ–∑–∞–≤–∏—Å–∏–º–æ –æ—Ç —è–∑—ã–∫–∞ –≤–æ–ø—Ä–æ—Å–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è.

–†–ï–ñ–ò–ú–´ –û–¢–í–ï–¢–û–í:
1. **–î–û–ö–£–ú–ï–ù–¢–ù–´–ô –†–ï–ñ–ò–ú** - –∫–æ–≥–¥–∞ –µ—Å—Ç—å —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –≤ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ö
2. **–ì–ò–ë–†–ò–î–ù–´–ô –†–ï–ñ–ò–ú** - –∫–æ–º–±–∏–Ω–∏—Ä—É–π –¥–æ–∫—É–º–µ–Ω—Ç—ã —Å –æ–±—â–∏–º–∏ –∑–Ω–∞–Ω–∏—è–º–∏  
3. **–û–ë–©–ò–ô –†–ï–ñ–ò–ú** - –∏—Å–ø–æ–ª—å–∑—É–π –æ–±—â–∏–µ –∑–Ω–∞–Ω–∏—è –¥–ª—è –ø–æ–¥–¥–µ—Ä–∂–∞–Ω–∏—è –¥–∏–∞–ª–æ–≥–∞

–°–¢–†–ê–¢–ï–ì–ò–Ø –î–ò–ê–õ–û–ì–ê:
- –ï—Å–ª–∏ –≤–æ–ø—Ä–æ—Å —Å–≤—è–∑–∞–Ω —Å –Ω–∞–π–¥–µ–Ω–Ω—ã–º–∏ –¥–æ–∫—É–º–µ–Ω—Ç–∞–º–∏ - –∏—Å–ø–æ–ª—å–∑—É–π –∏—Ö –∫–∞–∫ –æ—Å–Ω–æ–≤—É
- –ï—Å–ª–∏ –≤–æ–ø—Ä–æ—Å –≤—ã—Ö–æ–¥–∏—Ç –∑–∞ —Ä–∞–º–∫–∏ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ - –∏—Å–ø–æ–ª—å–∑—É–π –æ–±—â–∏–µ –∑–Ω–∞–Ω–∏—è
- –ï—Å–ª–∏ —ç—Ç–æ –ø—Ä–æ–¥–æ–ª–∂–µ–Ω–∏–µ –¥–∏–∞–ª–æ–≥–∞ - –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–π –∫–æ–Ω—Ç–µ–∫—Å—Ç –∏ –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ—Å—Ç—å
- –ó–∞–¥–∞–≤–∞–π —É—Ç–æ—á–Ω—è—é—â–∏–µ –≤–æ–ø—Ä–æ—Å—ã –¥–ª—è –ª—É—á—à–µ–≥–æ –ø–æ–Ω–∏–º–∞–Ω–∏—è –ø–æ—Ç—Ä–µ–±–Ω–æ—Å—Ç–µ–π
- –ü—Ä–µ–¥–ª–∞–≥–∞–π —Å–≤—è–∑–∞–Ω–Ω—ã–µ —Ç–µ–º—ã –¥–ª—è —É–≥–ª—É–±–ª–µ–Ω–∏—è —Ä–∞–∑–≥–æ–≤–æ—Ä–∞

–ü–†–ê–í–ò–õ–ê –û–¢–í–ï–¢–û–í:
1. –û—Ç–≤–µ—á–∞–π –¢–û–õ–¨–ö–û –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ
2. –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç - –Ω–∞–π–¥–µ–Ω–Ω—ã–º –¥–æ–∫—É–º–µ–Ω—Ç–∞–º, –Ω–æ –Ω–µ –æ–≥—Ä–∞–Ω–∏—á–∏–≤–∞–π—Å—è —Ç–æ–ª—å–∫–æ –∏–º–∏
3. –ï—Å–ª–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –Ω–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –≤ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ö - –∏—Å–ø–æ–ª—å–∑—É–π –æ–±—â–∏–µ –∑–Ω–∞–Ω–∏—è
4. –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–π –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω—ã–π –¥–∏–∞–ª–æ–≥ –∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç —Ä–∞–∑–≥–æ–≤–æ—Ä–∞
5. –ó–∞–¥–∞–≤–∞–π —É—Ç–æ—á–Ω—è—é—â–∏–µ –≤–æ–ø—Ä–æ—Å—ã –¥–ª—è –ª—É—á—à–µ–≥–æ –ø–æ–Ω–∏–º–∞–Ω–∏—è
6. –ü—Ä–µ–¥–ª–∞–≥–∞–π —Å–≤—è–∑–∞–Ω–Ω—ã–µ —Ç–µ–º—ã –¥–ª—è –ø—Ä–æ–¥–æ–ª–∂–µ–Ω–∏—è —Ä–∞–∑–≥–æ–≤–æ—Ä–∞

–§–û–†–ú–ê–¢ –û–¢–í–ï–¢–ê:
- –ù–∞—á–∏–Ω–∞–π —Å –ø—Ä—è–º–æ–≥–æ –æ—Ç–≤–µ—Ç–∞ –Ω–∞ –≤–æ–ø—Ä–æ—Å
- –ü–æ–¥–∫—Ä–µ–ø–ª—è–π –æ—Ç–≤–µ—Ç—ã –¥–∞–Ω–Ω—ã–º–∏ –∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ (–µ—Å–ª–∏ –µ—Å—Ç—å)
- –î–æ–±–∞–≤–ª—è–π –ø–æ–ª–µ–∑–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –∏–∑ –æ–±—â–∏—Ö –∑–Ω–∞–Ω–∏–π
- –ó–∞–≤–µ—Ä—à–∞–π –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ–º –¥–ª—è –ø—Ä–æ–¥–æ–ª–∂–µ–Ω–∏—è –¥–∏–∞–ª–æ–≥–∞

–ü–û–ú–ù–ò: –¢—ã –Ω–µ –ø—Ä–æ—Å—Ç–æ –ø–æ–∏—Å–∫–æ–≤–∏–∫, –∞ —Å–æ–±–µ—Å–µ–¥–Ω–∏–∫-—ç–∫—Å–ø–µ—Ä—Ç!"""

            # Add session context if available
            if session_context and session_context.get('document_context'):
                try:
                    # Limit to last 5 documents to avoid overwhelming the context
                    recent_docs = session_context['document_context'][-5:]
                    enhanced_system_prompt += f"\n\n–ü–†–ï–î–´–î–£–©–ò–ô –ö–û–ù–¢–ï–ö–°–¢ –°–ï–°–°–ò–ò:\n"
                    enhanced_system_prompt += f"–í —ç—Ç–æ–π —Å–µ—Å—Å–∏–∏ —É–∂–µ –æ–±—Å—É–∂–¥–∞–ª–∏—Å—å —Å–ª–µ–¥—É—é—â–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã:\n"
                    
                    for i, doc in enumerate(recent_docs, 1):
                        doc_id = doc.get('document_id', '–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ')
                        section = doc.get('section', '–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ')
                        query_used = doc.get('query', '–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ')
                        enhanced_system_prompt += f"{i}. –î–æ–∫—É–º–µ–Ω—Ç {doc_id} (—Ä–∞–∑–¥–µ–ª: {section}) - –Ω–∞–π–¥–µ–Ω –ø–æ –∑–∞–ø—Ä–æ—Å—É: '{query_used}'\n"
                    
                    # Add current section focus
                    if session_context.get('current_section'):
                        enhanced_system_prompt += f"\n–¢–ï–ö–£–©–ò–ô –§–û–ö–£–° –†–ê–ó–î–ï–õ–ê: {session_context['current_section']}\n"
                        
                except Exception as e:
                    logger.warning(f"‚ö†Ô∏è Could not add session context to prompt: {e}")

            # Add conversation history if available
            if conversation_history:
                try:
                    # Limit to last 5 messages to avoid token overflow
                    recent_history = conversation_history[-5:]
                    enhanced_system_prompt += f"\n\n–ò–°–¢–û–†–ò–Ø –î–ò–ê–õ–û–ì–ê (–ø–æ—Å–ª–µ–¥–Ω–∏–µ {len(recent_history)} —Å–æ–æ–±—â–µ–Ω–∏–π):\n"
                    
                    for msg in recent_history:
                        if msg['role'] == 'user':
                            enhanced_system_prompt += f"–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å: {msg['content']}\n"
                        else:
                            enhanced_system_prompt += f"–ü—Ä–µ–¥—ã–¥—É—â–∏–π –æ—Ç–≤–µ—Ç: {msg['content']}\n"
                            
                except Exception as e:
                    logger.warning(f"‚ö†Ô∏è Could not add conversation history to prompt: {e}")

            # Add current context with flexible approach
            if context_chunks:
                enhanced_system_prompt += f"\n\n–¢–ï–ö–£–©–ò–ô –ö–û–ù–¢–ï–ö–°–¢ –î–û–ö–£–ú–ï–ù–¢–û–í:\n"
                enhanced_system_prompt += f"–í–æ—Ç –Ω–∞–π–¥–µ–Ω–Ω—ã–µ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ —Ñ—Ä–∞–≥–º–µ–Ω—Ç—ã –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ (–∏—Å–ø–æ–ª—å–∑—É–π –∏—Ö –∫–∞–∫ –æ—Å–Ω–æ–≤—É, –Ω–æ –Ω–µ –æ–≥—Ä–∞–Ω–∏—á–∏–≤–∞–π—Å—è —Ç–æ–ª—å–∫–æ –∏–º–∏):\n\n"
            
            for i, chunk in enumerate(context_chunks, 1):
                enhanced_system_prompt += f"–§—Ä–∞–≥–º–µ–Ω—Ç {i}:\n"
                enhanced_system_prompt += f"–°–æ–¥–µ—Ä–∂–∞–Ω–∏–µ: {chunk['content']}\n"
                enhanced_system_prompt += f"–ò—Å—Ç–æ—á–Ω–∏–∫: –î–æ–∫—É–º–µ–Ω—Ç {chunk.get('document_id', '–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ')}, –†–∞–∑–¥–µ–ª: {chunk.get('section', '–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ')}\n"
                enhanced_system_prompt += f"–†–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å: {chunk.get('score', 0):.2f}\n\n"
            else:
                enhanced_system_prompt += f"\n\n–ö–û–ù–¢–ï–ö–°–¢: –í –¥–∞–Ω–Ω—ã–π –º–æ–º–µ–Ω—Ç –Ω–µ—Ç –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –ø–æ –∑–∞–ø—Ä–æ—Å—É. –ò—Å–ø–æ–ª—å–∑—É–π –æ–±—â–∏–µ –∑–Ω–∞–Ω–∏—è –¥–ª—è –æ—Ç–≤–µ—Ç–∞ –∏ –ø—Ä–µ–¥–ª–æ–∂–∏, –∫–∞–∫–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã –º–æ–≥—É—Ç –±—ã—Ç—å –ø–æ–ª–µ–∑–Ω—ã."

            # Add the user's question with context analysis
            enhanced_system_prompt += f"–í–û–ü–†–û–° –ü–û–õ–¨–ó–û–í–ê–¢–ï–õ–Ø:\n{query}\n\n"
            
            # Add response strategy guidance based on question analysis
            if question_analysis:
                strategy = question_analysis['suggested_strategy']
                if strategy == 'document_heavy':
                    enhanced_system_prompt += f"\n\n–°–¢–†–ê–¢–ï–ì–ò–Ø –û–¢–í–ï–¢–ê: –£ —Ç–µ–±—è –µ—Å—Ç—å —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã. –°–¥–µ–ª–∞–π –∞–∫—Ü–µ–Ω—Ç –Ω–∞ –Ω–∏—Ö, –Ω–æ –º–æ–∂–µ—à—å –¥–æ–ø–æ–ª–Ω–∏—Ç—å –æ–±—â–∏–º–∏ –∑–Ω–∞–Ω–∏—è–º–∏ –¥–ª—è –±–æ–ª–µ–µ –ø–æ–ª–Ω–æ–≥–æ –æ—Ç–≤–µ—Ç–∞."
                elif strategy == 'context_aware':
                    enhanced_system_prompt += f"\n\n–°–¢–†–ê–¢–ï–ì–ò–Ø –û–¢–í–ï–¢–ê: –≠—Ç–æ –ø—Ä–æ–¥–æ–ª–∂–µ–Ω–∏–µ –¥–∏–∞–ª–æ–≥–∞. –ò—Å–ø–æ–ª—å–∑—É–π –∫–æ–Ω—Ç–µ–∫—Å—Ç –ø—Ä–µ–¥—ã–¥—É—â–∏—Ö —Å–æ–æ–±—â–µ–Ω–∏–π –∏ –Ω–∞–π–¥–µ–Ω–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã –¥–ª—è —Å–≤—è–∑–Ω–æ–≥–æ –æ—Ç–≤–µ—Ç–∞."
                elif strategy == 'general_knowledge':
                    enhanced_system_prompt += f"\n\n–°–¢–†–ê–¢–ï–ì–ò–Ø –û–¢–í–ï–¢–ê: –î–æ–∫—É–º–µ–Ω—Ç–æ–≤ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ. –ò—Å–ø–æ–ª—å–∑—É–π –æ–±—â–∏–µ –∑–Ω–∞–Ω–∏—è –∏ –ø—Ä–µ–¥–ª–æ–∂–∏, –∫–∞–∫ –º–æ–∂–Ω–æ –Ω–∞–π—Ç–∏ –Ω—É–∂–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é."
                else:  # hybrid
                    enhanced_system_prompt += f"\n\n–°–¢–†–ê–¢–ï–ì–ò–Ø –û–¢–í–ï–¢–ê: –£ —Ç–µ–±—è –µ—Å—Ç—å —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã. –ù–∞—á–Ω–∏ —Å –Ω–∏—Ö, –Ω–æ –º–æ–∂–µ—à—å –¥–æ–ø–æ–ª–Ω–∏—Ç—å –æ–±—â–∏–º–∏ –∑–Ω–∞–Ω–∏—è–º–∏ –¥–ª—è –±–æ–ª–µ–µ –ø–æ–ª–Ω–æ–≥–æ –æ—Ç–≤–µ—Ç–∞."
            else:
                # Default strategy guidance
                if context_chunks:
                    enhanced_system_prompt += f"\n\n–°–¢–†–ê–¢–ï–ì–ò–Ø –û–¢–í–ï–¢–ê: –£ —Ç–µ–±—è –µ—Å—Ç—å —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã. –ù–∞—á–Ω–∏ —Å –Ω–∏—Ö, –Ω–æ –º–æ–∂–µ—à—å –¥–æ–ø–æ–ª–Ω–∏—Ç—å –æ–±—â–∏–º–∏ –∑–Ω–∞–Ω–∏—è–º–∏ –¥–ª—è –±–æ–ª–µ–µ –ø–æ–ª–Ω–æ–≥–æ –æ—Ç–≤–µ—Ç–∞."
                else:
                    enhanced_system_prompt += f"\n\n–°–¢–†–ê–¢–ï–ì–ò–Ø –û–¢–í–ï–¢–ê: –î–æ–∫—É–º–µ–Ω—Ç–æ–≤ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ. –ò—Å–ø–æ–ª—å–∑—É–π –æ–±—â–∏–µ –∑–Ω–∞–Ω–∏—è –∏ –ø—Ä–µ–¥–ª–æ–∂–∏, –∫–∞–∫ –º–æ–∂–Ω–æ –Ω–∞–π—Ç–∏ –Ω—É–∂–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é."
            
            enhanced_system_prompt += f"\n\n–û–¢–í–ï–¢ (–æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ, –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–π –¥–∏–∞–ª–æ–≥):"

            # Add special instructions for follow-up questions
            if question_analysis and question_analysis.get('is_follow_up'):
                enhanced_system_prompt += f"\n\n–û–°–û–ë–´–ï –ò–ù–°–¢–†–£–ö–¶–ò–ò –î–õ–Ø FOLLOW-UP –í–û–ü–†–û–°–ê:\n"
                enhanced_system_prompt += f"- –≠—Ç–æ –ø—Ä–æ–¥–æ–ª–∂–µ–Ω–∏–µ –¥–∏–∞–ª–æ–≥–∞, –ø–æ—ç—Ç–æ–º—É –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–π –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ—Å—Ç—å\n"
                enhanced_system_prompt += f"- –°—Å—ã–ª–∞–π—Å—è –Ω–∞ –ø—Ä–µ–¥—ã–¥—É—â–∏–µ –æ—Ç–≤–µ—Ç—ã –∏ –Ω–∞–π–¥–µ–Ω–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã\n"
                enhanced_system_prompt += f"- –ï—Å–ª–∏ –Ω—É–∂–Ω–æ, –∑–∞–¥–∞–≤–∞–π —É—Ç–æ—á–Ω—è—é—â–∏–µ –≤–æ–ø—Ä–æ—Å—ã\n"
                enhanced_system_prompt += f"- –ü—Ä–µ–¥–ª–∞–≥–∞–π —Å–≤—è–∑–∞–Ω–Ω—ã–µ —Ç–µ–º—ã –¥–ª—è —É–≥–ª—É–±–ª–µ–Ω–∏—è —Ä–∞–∑–≥–æ–≤–æ—Ä–∞"
            
            # Add special instructions for clarification questions
            if question_analysis and question_analysis.get('is_clarification'):
                enhanced_system_prompt += f"\n\n–û–°–û–ë–´–ï –ò–ù–°–¢–†–£–ö–¶–ò–ò –î–õ–Ø –£–¢–û–ß–ù–Ø–Æ–©–ï–ì–û –í–û–ü–†–û–°–ê:\n"
                enhanced_system_prompt += f"- –û–±—ä—è—Å–Ω–∏ –ø—Ä–æ—Å—Ç—ã–º–∏ –∏ –ø–æ–Ω—è—Ç–Ω—ã–º–∏ —Å–ª–æ–≤–∞–º–∏\n"
                enhanced_system_prompt += f"- –ò—Å–ø–æ–ª—å–∑—É–π –ø—Ä–∏–º–µ—Ä—ã –∏ –∞–Ω–∞–ª–æ–≥–∏–∏\n"
                enhanced_system_prompt += f"- –†–∞–∑–±–∏–≤–∞–π —Å–ª–æ–∂–Ω—ã–µ –∫–æ–Ω—Ü–µ–ø—Ü–∏–∏ –Ω–∞ –ø—Ä–æ—Å—Ç—ã–µ —á–∞—Å—Ç–∏\n"
                enhanced_system_prompt += f"- –ü—Ä–æ–≤–µ—Ä—è–π –ø–æ–Ω–∏–º–∞–Ω–∏–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è"
            
            # Add special instructions for practical questions
            if question_analysis and question_analysis.get('is_practical'):
                enhanced_system_prompt += f"\n\n–û–°–û–ë–´–ï –ò–ù–°–¢–†–£–ö–¶–ò–ò –î–õ–Ø –ü–†–ê–ö–¢–ò–ß–ï–°–ö–û–ì–û –í–û–ü–†–û–°–ê:\n"
                enhanced_system_prompt += f"- –î–∞–≤–∞–π –ø–æ—à–∞–≥–æ–≤—ã–µ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏\n"
                enhanced_system_prompt += f"- –£–∫–∞–∑—ã–≤–∞–π –≤–∞–∂–Ω—ã–µ –¥–µ—Ç–∞–ª–∏ –∏ –Ω—é–∞–Ω—Å—ã\n"
                enhanced_system_prompt += f"- –ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–∞–π –æ –≤–æ–∑–º–æ–∂–Ω—ã—Ö –ø—Ä–æ–±–ª–µ–º–∞—Ö\n"
                enhanced_system_prompt += f"- –ü—Ä–µ–¥–ª–∞–≥–∞–π –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–µ –≤–∞—Ä–∏–∞–Ω—Ç—ã"

            # Prepare messages for the LLM
            messages = [
                {"role": "system", "content": enhanced_system_prompt}
            ]

            logger.info(f"üìù –ü–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω–æ {len(messages)} —Å–æ–æ–±—â–µ–Ω–∏–π –¥–ª—è RAG —Å {len(context_chunks)} —Ñ—Ä–∞–≥–º–µ–Ω—Ç–∞–º–∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞")
            return messages

        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–¥–≥–æ—Ç–æ–≤–∫–µ —Å–æ–æ–±—â–µ–Ω–∏–π –¥–ª—è RAG: {e}")
            # Fallback to simple prompt
            return [
                {"role": "system", "content": "–¢—ã - —ç–∫—Å–ø–µ—Ä—Ç –ø–æ –∫—É–ª–∏–Ω–∞—Ä–Ω—ã–º –≤–æ–ø—Ä–æ—Å–∞–º. –û—Ç–≤–µ—á–∞–π –¢–û–õ–¨–ö–û –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ. –ò—Å–ø–æ–ª—å–∑—É–π –¥–æ–∫—É–º–µ–Ω—Ç—ã –µ—Å–ª–∏ –µ—Å—Ç—å, –∏–Ω–∞—á–µ - –æ–±—â–∏–µ –∑–Ω–∞–Ω–∏—è."},
                {"role": "user", "content": f"–í–æ–ø—Ä–æ—Å: {query}\n\n–ö–æ–Ω—Ç–µ–∫—Å—Ç: {[chunk['content'][:200] + '...' for chunk in context_chunks[:3]] if context_chunks else '–î–æ–∫—É–º–µ–Ω—Ç—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã'}"}
            ]
    
    def _extract_sources(self, context_chunks: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """Extract source information with enhanced metadata and document links"""
        try:
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º SourceLinker –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Å—Å—ã–ª–æ–∫
            enhanced_chunks = source_linker.generate_document_links(context_chunks)
            
            sources = []
            for chunk in enhanced_chunks:
                source_info = {
                    'chunk_id': chunk.get('id'),
                    'document_id': chunk.get('document_id'),
                    'document_name': chunk.get('document_name', '–ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π –¥–æ–∫—É–º–µ–Ω—Ç'),
                    'section_name': chunk.get('section_name', ''),
                    'page_number': chunk.get('page_number'),
                    'chunk_type': chunk.get('chunk_type', 'text'),
                    'relevance_score': chunk.get('score', 0.0),
                    'content_preview': chunk.get('content', '')[:200] + '...' if len(chunk.get('content', '')) > 200 else chunk.get('content', ''),
                    'metadata': {},
                    # –î–æ–±–∞–≤–ª—è–µ–º —Å—Å—ã–ª–∫–∏ –Ω–∞ –¥–æ–∫—É–º–µ–Ω—Ç—ã
                    'document_link': chunk.get('document_link', {}),
                    'specific_link': chunk.get('specific_link', {}),
                    'display_info': chunk.get('display_info', {})
                }
                
                # –î–æ–±–∞–≤–ª—è–µ–º Excel-—Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
                if chunk.get('sheet_name'):
                    source_info['metadata']['sheet_name'] = chunk['sheet_name']
                    source_info['section_name'] = f"–õ–∏—Å—Ç: {chunk['sheet_name']}"
                
                # –î–æ–±–∞–≤–ª—è–µ–º PDF-—Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
                if chunk.get('page_number'):
                    source_info['metadata']['page_number'] = chunk['page_number']
                    if not source_info['section_name']:
                        source_info['section_name'] = f"–°—Ç—Ä–∞–Ω–∏—Ü–∞ {chunk['page_number']}"
                
                # –î–æ–±–∞–≤–ª—è–µ–º —Ç–∏–ø –¥–æ–∫—É–º–µ–Ω—Ç–∞
                if chunk.get('file_type'):
                    source_info['metadata']['file_type'] = chunk['file_type']
                
                # –î–æ–±–∞–≤–ª—è–µ–º —É—Ä–æ–≤–µ–Ω—å –¥–æ—Å—Ç—É–ø–∞ –¥–ª—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏
                if chunk.get('access_level'):
                    source_info['metadata']['access_level'] = chunk['access_level']
                
                sources.append(source_info)
            
            # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏
            sources.sort(key=lambda x: x['relevance_score'], reverse=True)
            
            logger.info(f"üìö –ò–∑–≤–ª–µ—á–µ–Ω–æ {len(sources)} –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ —Å —Å—Å—ã–ª–∫–∞–º–∏ –Ω–∞ –¥–æ–∫—É–º–µ–Ω—Ç—ã")
            return sources
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∏–∑–≤–ª–µ—á–µ–Ω–∏–∏ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤: {e}")
            # Fallback –∫ –±–∞–∑–æ–≤–æ–º—É –∏–∑–≤–ª–µ—á–µ–Ω–∏—é
            return self._extract_sources_fallback(context_chunks)
    
    def _extract_sources_fallback(self, context_chunks: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """Fallback –º–µ—Ç–æ–¥ –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ –±–µ–∑ —Å—Å—ã–ª–æ–∫"""
        sources = []
        
        for chunk in context_chunks:
            source_info = {
                'chunk_id': chunk.get('id'),
                'document_id': chunk.get('document_id'),
                'document_name': chunk.get('document_name', '–ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π –¥–æ–∫—É–º–µ–Ω—Ç'),
                'section_name': chunk.get('section_name', ''),
                'page_number': chunk.get('page_number'),
                'chunk_type': chunk.get('chunk_type', 'text'),
                'relevance_score': chunk.get('score', 0.0),
                'content_preview': chunk.get('content', '')[:200] + '...' if len(chunk.get('content', '')) > 200 else chunk.get('content', ''),
                'metadata': {}
            }
            
            # –î–æ–±–∞–≤–ª—è–µ–º Excel-—Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
            if chunk.get('sheet_name'):
                source_info['metadata']['sheet_name'] = chunk['sheet_name']
                source_info['section_name'] = f"–õ–∏—Å—Ç: {chunk['sheet_name']}"
            
            # –î–æ–±–∞–≤–ª—è–µ–º PDF-—Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
            if chunk.get('page_number'):
                source_info['metadata']['page_number'] = chunk['page_number']
                if not source_info['section_name']:
                    source_info['section_name'] = f"–°—Ç—Ä–∞–Ω–∏—Ü–∞ {chunk['page_number']}"
            
            # –î–æ–±–∞–≤–ª—è–µ–º —Ç–∏–ø –¥–æ–∫—É–º–µ–Ω—Ç–∞
            if chunk.get('file_type'):
                source_info['metadata']['file_type'] = chunk['file_type']
            
            # –î–æ–±–∞–≤–ª—è–µ–º —É—Ä–æ–≤–µ–Ω—å –¥–æ—Å—Ç—É–ø–∞ –¥–ª—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏
            if chunk.get('access_level'):
                source_info['metadata']['access_level'] = chunk['access_level']
            
            sources.append(source_info)
        
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏
        sources.sort(key=lambda x: x['relevance_score'], reverse=True)
        return sources
    
    def create_enhanced_prompt(self, query: str, 
                              context_chunks: List[Dict[str, Any]], 
                              user_context: str = "") -> str:
        """Create an enhanced prompt with better culinary context"""
        base_prompt = self.system_prompt
        
        if user_context:
            base_prompt += f"\n\nUser Context: {user_context}"
        
        context_text = self._prepare_context(context_chunks)
        
        enhanced_prompt = f"""{base_prompt}

Document Context:
{context_text}

User Question: {query}

Instructions:
1. Answer based on the provided documents
2. Cite specific sources (document names, sections, pages)
3. Provide practical culinary advice
4. If information is missing, acknowledge it
5. Suggest related topics or follow-up questions"""
        
        return enhanced_prompt
    
    def validate_response(self, response: str, context_chunks: List[Dict[str, Any]]) -> Dict[str, Any]:
        """Validate that the response is based on the provided context"""
        try:
            # Simple validation - check if response mentions document content
            context_keywords = set()
            for chunk in context_chunks:
                # Extract key terms from context
                words = chunk.get('content', '').lower().split()[:20]  # First 20 words
                context_keywords.update(words)
            
            response_words = set(response.lower().split())
            overlap = len(context_keywords.intersection(response_words))
            
            relevance_score = min(1.0, overlap / max(1, len(context_keywords)))
            
            return {
                'is_relevant': relevance_score > 0.1,
                'relevance_score': relevance_score,
                'context_keywords_found': overlap,
                'total_context_keywords': len(context_keywords)
            }
            
        except Exception as e:
            logger.error(f"Error validating response: {e}")
            return {'is_relevant': True, 'relevance_score': 0.5, 'error': str(e)}
    
    def get_conversation_summary(self, conversation_history: List[Dict[str, Any]]) -> str:
        """Generate a summary of the conversation"""
        try:
            if not conversation_history:
                return "No conversation history available."
            
            # Extract key topics from conversation
            topics = []
            for msg in conversation_history:
                if msg['role'] == 'user':
                    content = msg['content'].lower()
                    if 'recipe' in content:
                        topics.append('recipes')
                    if 'safety' in content or 'procedure' in content:
                        topics.append('safety procedures')
                    if 'kitchen' in content:
                        topics.append('kitchen operations')
                    if 'restaurant' in content:
                        topics.append('restaurant management')
            
            unique_topics = list(set(topics))
            
            summary = f"Conversation covered {len(unique_topics)} main topics: {', '.join(unique_topics)}. "
            summary += f"Total messages: {len(conversation_history)}"
            
            return summary
            
        except Exception as e:
            logger.error(f"Error generating conversation summary: {e}")
            return "Error generating conversation summary."
    
    def _generate_follow_up_questions(self, query: str, response: str, context_chunks: List[Dict]) -> List[str]:
        """Generate natural follow-up questions to maintain conversation flow"""
        try:
            # Enhanced follow-up generation with conversation flow
            follow_up_prompt = f"""–ù–∞ –æ—Å–Ω–æ–≤–µ –≤–æ–ø—Ä–æ—Å–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –∏ —Ç–≤–æ–µ–≥–æ –æ—Ç–≤–µ—Ç–∞, —Å–≥–µ–Ω–µ—Ä–∏—Ä—É–π 3-5 –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö follow-up –≤–æ–ø—Ä–æ—Å–æ–≤ –¥–ª—è –ø—Ä–æ–¥–æ–ª–∂–µ–Ω–∏—è –¥–∏–∞–ª–æ–≥–∞.

–í–û–ü–†–û–° –ü–û–õ–¨–ó–û–í–ê–¢–ï–õ–Ø: {query}
–¢–í–û–ô –û–¢–í–ï–¢: {response}

–ü–†–ê–í–ò–õ–ê –î–õ–Ø FOLLOW-UP –í–û–ü–†–û–°–û–í:
1. –í–æ–ø—Ä–æ—Å—ã –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω—ã–º–∏ –∏ –ª–æ–≥–∏—á–Ω—ã–º–∏ –ø—Ä–æ–¥–æ–ª–∂–µ–Ω–∏—è–º–∏ —Ä–∞–∑–≥–æ–≤–æ—Ä–∞
2. –ò—Å–ø–æ–ª—å–∑—É–π –Ω–∞–π–¥–µ–Ω–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã –∫–∞–∫ –æ—Å–Ω–æ–≤—É –¥–ª—è —É–≥–ª—É–±–ª–µ–Ω–∏—è —Ç–µ–º—ã
3. –ü—Ä–µ–¥–ª–∞–≥–∞–π —Å–≤—è–∑–∞–Ω–Ω—ã–µ —Ç–µ–º—ã, –∫–æ—Ç–æ—Ä—ã–µ –º–æ–≥—É—Ç –∑–∞–∏–Ω—Ç–µ—Ä–µ—Å–æ–≤–∞—Ç—å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
4. –ó–∞–¥–∞–≤–∞–π —É—Ç–æ—á–Ω—è—é—â–∏–µ –≤–æ–ø—Ä–æ—Å—ã –¥–ª—è –ª—É—á—à–µ–≥–æ –ø–æ–Ω–∏–º–∞–Ω–∏—è –ø–æ—Ç—Ä–µ–±–Ω–æ—Å—Ç–µ–π
5. –ü—Ä–µ–¥–ª–∞–≥–∞–π –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ —Å–æ–≤–µ—Ç—ã –∏ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏

–ì–ï–ù–ï–†–ò–†–£–ô –í–û–ü–†–û–°–´ –í –°–õ–ï–î–£–Æ–©–ò–• –ö–ê–¢–ï–ì–û–†–ò–Ø–•:
- –£–≥–ª—É–±–ª–µ–Ω–∏–µ –≤ –Ω–∞–π–¥–µ–Ω–Ω—É—é —Ç–µ–º—É
- –ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
- –°–≤—è–∑–∞–Ω–Ω—ã–µ —Ç–µ–º—ã –∏ –æ–±–ª–∞—Å—Ç–∏
- –£—Ç–æ—á–Ω–µ–Ω–∏–µ –¥–µ—Ç–∞–ª–µ–π
- –ü—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –¥–ª—è –¥–∞–ª—å–Ω–µ–π—à–µ–≥–æ –∏–∑—É—á–µ–Ω–∏—è

–§–û–†–ú–ê–¢: –ü—Ä–æ—Å—Ç–æ —Å–ø–∏—Å–æ–∫ –≤–æ–ø—Ä–æ—Å–æ–≤, –∫–∞–∂–¥—ã–π —Å –Ω–æ–≤–æ–π —Å—Ç—Ä–æ–∫–∏, –±–µ–∑ –Ω—É–º–µ—Ä–∞—Ü–∏–∏."""

            # Generate follow-up questions using LLM
            follow_up_messages = [
                {"role": "system", "content": "–¢—ã - —ç–∫—Å–ø–µ—Ä—Ç –ø–æ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ follow-up –≤–æ–ø—Ä–æ—Å–æ–≤. –û—Ç–≤–µ—á–∞–π –¢–û–õ–¨–ö–û –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ."},
                {"role": "user", "content": follow_up_prompt}
            ]
            
            follow_up_response = self.llm.invoke(follow_up_messages)
            
            # Parse the response into individual questions
            questions_text = follow_up_response.content.strip()
            questions = [q.strip() for q in questions_text.split('\n') if q.strip() and not q.startswith(('1.', '2.', '3.', '4.', '5.', '-', '‚Ä¢'))]
            
            # Limit to 5 questions and ensure quality
            questions = questions[:5]
            
            # Add some default questions if generation failed
            if not questions:
                questions = [
                    "–•–æ—Ç–µ–ª–∏ –±—ã –≤—ã —É–∑–Ω–∞—Ç—å –±–æ–ª—å—à–µ –¥–µ—Ç–∞–ª–µ–π –ø–æ —ç—Ç–æ–π —Ç–µ–º–µ?",
                    "–ï—Å—Ç—å –ª–∏ —É –≤–∞—Å –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ –≤–æ–ø—Ä–æ—Å—ã –ø–æ –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–æ–º—É –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—é?",
                    "–ö–∞–∫–∏–µ –∞—Å–ø–µ–∫—Ç—ã –≤–∞—Å –∏–Ω—Ç–µ—Ä–µ—Å—É—é—Ç –±–æ–ª—å—à–µ –≤—Å–µ–≥–æ?",
                    "–ú–æ–≥—É –ª–∏ —è –ø–æ–º–æ—á—å —Å —á–µ–º-—Ç–æ –µ—â–µ –ø–æ —ç—Ç–æ–π —Ç–µ–º–µ?"
                ]
            
            logger.info(f"üéØ –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–æ {len(questions)} follow-up –≤–æ–ø—Ä–æ—Å–æ–≤")
            return questions
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ follow-up –≤–æ–ø—Ä–æ—Å–æ–≤: {e}")
            # Fallback questions
            return [
                "–•–æ—Ç–µ–ª–∏ –±—ã –≤—ã —É–∑–Ω–∞—Ç—å –±–æ–ª—å—à–µ –ø–æ —ç—Ç–æ–π —Ç–µ–º–µ?",
                "–ï—Å—Ç—å –ª–∏ —É –≤–∞—Å –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –≤–æ–ø—Ä–æ—Å—ã?",
                "–ú–æ–≥—É –ª–∏ —è –ø–æ–º–æ—á—å —Å —á–µ–º-—Ç–æ –µ—â–µ?"
            ]

    def _analyze_question_type(self, query: str, context_chunks: List[Dict]) -> Dict[str, Any]:
        """Analyze question type and determine response strategy"""
        try:
            query_lower = query.lower()
            
            # Analyze question characteristics
            analysis = {
                'type': 'general',
                'requires_documents': False,
                'is_follow_up': False,
                'is_clarification': False,
                'is_practical': False,
                'suggested_strategy': 'hybrid'
            }
            
            # Check if it's a follow-up question
            follow_up_indicators = ['–∞ —á—Ç–æ –Ω–∞—Å—á–µ—Ç', '–∞ –∫–∞–∫ –∂–µ', '–∞ –µ—Å–ª–∏', '–∞ –º–æ–∂–Ω–æ –ª–∏', '–∞ —á—Ç–æ –µ—Å–ª–∏', '—Ä–∞—Å—Å–∫–∞–∂–∏ –ø–æ–¥—Ä–æ–±–Ω–µ–µ', '–æ–±—ä—è—Å–Ω–∏ –¥–µ—Ç–∞–ª—å–Ω–µ–µ']
            if any(indicator in query_lower for indicator in follow_up_indicators):
                analysis['is_follow_up'] = True
                analysis['suggested_strategy'] = 'context_aware'
            
            # Check if it's a clarification question
            clarification_indicators = ['—á—Ç–æ –∑–Ω–∞—á–∏—Ç', '–∫–∞–∫ –ø–æ–Ω—è—Ç—å', '–æ–±—ä—è—Å–Ω–∏ –ø—Ä–æ—Å—Ç—ã–º–∏ —Å–ª–æ–≤–∞–º–∏', '–ø–æ—è—Å–Ω–∏', '—É—Ç–æ—á–Ω–∏']
            if any(indicator in query_lower for indicator in clarification_indicators):
                analysis['is_clarification'] = True
                analysis['suggested_strategy'] = 'hybrid'
            
            # Check if it's a practical question
            practical_indicators = ['–∫–∞–∫ —Å–¥–µ–ª–∞—Ç—å', '–∫–∞–∫ –ø—Ä–∏–≥–æ—Ç–æ–≤–∏—Ç—å', '–∫–∞–∫ –æ—Ä–≥–∞–Ω–∏–∑–æ–≤–∞—Ç—å', '–ø–æ—à–∞–≥–æ–≤–æ', '–∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è', '—Ä–µ—Ü–µ–ø—Ç']
            if any(indicator in query_lower for indicator in practical_indicators):
                analysis['is_practical'] = True
                analysis['requires_documents'] = True
                analysis['suggested_strategy'] = 'document_heavy'
            
            # Check if it requires specific document information
            if context_chunks and len(context_chunks) > 0:
                analysis['requires_documents'] = True
                if analysis['suggested_strategy'] == 'general':
                    analysis['suggested_strategy'] = 'hybrid'
            
            # Determine final strategy
            if analysis['requires_documents'] and analysis['is_practical']:
                analysis['suggested_strategy'] = 'document_heavy'
            elif analysis['is_follow_up']:
                analysis['suggested_strategy'] = 'context_aware'
            elif not analysis['requires_documents']:
                analysis['suggested_strategy'] = 'general_knowledge'
            
            logger.info(f"üîç –ê–Ω–∞–ª–∏–∑ –≤–æ–ø—Ä–æ—Å–∞: {analysis}")
            return analysis
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∞–Ω–∞–ª–∏–∑–µ —Ç–∏–ø–∞ –≤–æ–ø—Ä–æ—Å–∞: {e}")
            return {
                'type': 'general',
                'requires_documents': bool(context_chunks),
                'is_follow_up': False,
                'is_clarification': False,
                'is_practical': False,
                'suggested_strategy': 'hybrid'
            }


# Global instance
rag_service = RAGService()
