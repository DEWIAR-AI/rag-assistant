#!/usr/bin/env python3
"""
RAG System - Restaurant Management AI Assistant

–í–†–ï–ú–ï–ù–ù–´–ï –ò–ó–ú–ï–ù–ï–ù–ò–Ø (–¥–ª—è –∑–∞–∫–∞–∑—á–∏–∫–æ–≤):
- üîì –û—Ç–∫–ª—é—á–µ–Ω—ã –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è –ø–æ —Ä–æ–ª—è–º –¥–ª—è /chat —ç–Ω–¥–ø–æ–∏–Ω—Ç–∞
- ‚úÖ –í—Å–µ —Ä–æ–ª–∏ –º–æ–≥—É—Ç –æ–±—Ä–∞—â–∞—Ç—å—Å—è –∫–æ –≤—Å–µ–º —Ä–∞–∑–¥–µ–ª–∞–º (restaurant_ops, procedures, standards)
- üîí –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è –Ω–∞ –∑–∞–≥—Ä—É–∑–∫—É –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –°–û–•–†–ê–ù–ï–ù–´
- üìù –≠–Ω–¥–ø–æ–∏–Ω—Ç /documents/upload-smart –ø–æ-–ø—Ä–µ–∂–Ω–µ–º—É –ø—Ä–æ–≤–µ—Ä—è–µ—Ç –ø—Ä–∞–≤–∞ –¥–æ—Å—Ç—É–ø–∞

–ü–†–ò–ú–ï–ß–ê–ù–ò–ï: –≠—Ç–æ –≤—Ä–µ–º–µ–Ω–Ω–æ–µ —Ä–µ—à–µ–Ω–∏–µ –¥–æ –≥–æ—Ç–æ–≤–Ω–æ—Å—Ç–∏ —Å–∏—Å—Ç–µ–º—ã —Ä–∞–∑–¥–µ–ª–µ–Ω–∏—è –ø–æ —Ä–æ–ª—è–º —É –∑–∞–∫–∞–∑—á–∏–∫–æ–≤
"""

import asyncio
import json
import logging
import os
import uuid
from datetime import datetime
from typing import List, Optional

from fastapi import FastAPI, HTTPException, Depends, status, UploadFile, File, Form
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import JSONResponse
from fastapi.security import HTTPBearer, HTTPAuthorizationCredentials
# from fastapi.staticfiles import StaticFiles  # –£–±—Ä–∞–ª–∏, –Ω–µ –Ω—É–∂–µ–Ω
import uvicorn
from typing import List, Optional, Dict, Any
import os
import uuid
import asyncio
from datetime import datetime, timedelta
import json

from config import settings
from database.database import SessionLocal
from database.models import Document, AccessToken, Conversation, ConversationMessage
from services import (
    rag_service,
    vector_service,
    supabase_service,
    document_processor,
    auth_service,
    admin_service,
    session_context_service,
    embedding_service
)
from services.source_linker import source_linker
from services.rate_limiter import check_rate_limit_middleware
from services.auth_dependencies import get_current_token, get_admin_token
from services.cache_cleanup_router import router as cache_cleanup_router
from services.document_viewer_router import router as document_viewer_router
from schemas import (
    DocumentCreate, DocumentResponse, DocumentList, DocumentUpdate,
    ChatRequest, ChatResponse, ConversationCreate, ConversationResponse,
    AccessTokenCreate, AccessTokenResponse, TokenValidation, DocumentSearchRequest,
    DocumentSearchResult, UserRegister, UserLogin, AuthResponse
)
from sqlalchemy import text

# Configure logging
logging.basicConfig(
    level=getattr(logging, settings.log_level),
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)





# Security moved to auth_dependencies.py

# Database
def get_db():
    db = SessionLocal()
    try:
        yield db
    finally:
        db.close()

def get_db_session():
    """Get a single database session (not a generator)"""
    return SessionLocal()

# Lifespan context manager
from contextlib import asynccontextmanager

@asynccontextmanager
async def lifespan(app: FastAPI):
    """Lifespan context manager for startup and shutdown events"""
    # Startup
    try:
        logger.info("üöÄ –ó–∞–ø—É—Å–∫–∞–µ–º RAG –°–∏—Å—Ç–µ–º—É...")
        
        # Check if running in demo mode
        if settings.demo_mode:
            logger.info("üé≠ –ó–∞–ø—É—Å–∫ –≤ –¥–µ–º–æ-—Ä–µ–∂–∏–º–µ - –ø—Ä–æ–ø—É—Å–∫–∞–µ–º –ø—Ä–æ–≤–µ—Ä–∫–∏ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö")
        else:
            # Test database connection
            db = get_db_session()
            try:
                db.execute(text("SELECT 1"))
                logger.info("‚úÖ –ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ –±–∞–∑–µ –¥–∞–Ω–Ω—ã—Ö —É—Å–ø–µ—à–Ω–æ")
            except Exception as e:
                logger.error(f"‚ùå –ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ –±–∞–∑–µ –¥–∞–Ω–Ω—ã—Ö –Ω–µ —É–¥–∞–ª–æ—Å—å: {e}")
                raise
            finally:
                db.close()
            
            # Test Supabase connection
            try:
                # Test by checking storage usage
                storage_info = supabase_service.check_storage_usage()
                logger.info("‚úÖ –ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ Supabase —É—Å–ø–µ—à–Ω–æ")
            except Exception as e:
                logger.error(f"‚ùå –ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ Supabase –Ω–µ —É–¥–∞–ª–æ—Å—å: {e}")
                raise
        
        # Test OpenAI connection
        try:
            # Simple test - just check if the key is valid format
            if not settings.openai_api_key or len(settings.openai_api_key) < 20:
                raise ValueError("–ù–µ–≤–µ—Ä–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç –∫–ª—é—á–∞ OpenAI API")
            logger.info("‚úÖ –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è OpenAI –≤–∞–ª–∏–¥–Ω–∞")
        except Exception as e:
            logger.error(f"‚ùå –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è OpenAI –Ω–µ —É–¥–∞–ª–∞—Å—å: {e}")
            raise
        
        # Mount static files - –£–ë–ò–†–ê–ï–ú, –ù–ï –ù–£–ñ–ï–ù –î–õ–Ø API
        # app.mount("/static", StaticFiles(directory="static"), name="static")
        
        # –ó–∞–ø—É—Å–∫–∞–µ–º —Å–µ—Ä–≤–∏—Å –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–π –æ—á–∏—Å—Ç–∫–∏ –∫—ç—à–∞
        try:
            from services.cache_cleanup_service import cache_cleanup_service
            cache_cleanup_service.start()
            logger.info("üßπ –°–µ—Ä–≤–∏—Å –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–π –æ—á–∏—Å—Ç–∫–∏ –∫—ç—à–∞ –∑–∞–ø—É—â–µ–Ω")
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–ø—É—Å—Ç–∏—Ç—å —Å–µ—Ä–≤–∏—Å –æ—á–∏—Å—Ç–∫–∏ –∫—ç—à–∞: {e}")
        
        logger.info("üöÄ RAG –°–∏—Å—Ç–µ–º–∞ —É—Å–ø–µ—à–Ω–æ –∑–∞–ø—É—â–µ–Ω–∞!")
        
    except Exception as e:
        logger.error(f"‚ùå –ó–∞–ø—É—Å–∫ –Ω–µ —É–¥–∞–ª—Å—è: {e}")
        raise
    
    yield
    
    # Shutdown
    logger.info("üîÑ –í—ã–∫–ª—é—á–∞–µ–º RAG –°–∏—Å—Ç–µ–º—É...")
    
    # –û—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º —Å–µ—Ä–≤–∏—Å –æ—á–∏—Å—Ç–∫–∏ –∫—ç—à–∞
    try:
        from services.cache_cleanup_service import cache_cleanup_service
        if cache_cleanup_service.is_running:
            cache_cleanup_service.stop()
            logger.info("üßπ –°–µ—Ä–≤–∏—Å –æ—á–∏—Å—Ç–∫–∏ –∫—ç—à–∞ –æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω")
    except Exception as e:
        logger.warning(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –æ—Å—Ç–∞–Ω–æ–≤–∏—Ç—å —Å–µ—Ä–≤–∏—Å –æ—á–∏—Å—Ç–∫–∏ –∫—ç—à–∞: {e}")

# Main FastAPI application
app = FastAPI(
    title="RAG System API",
    description="Retrieval Augmented Generation System for Document Processing and AI Chat",
    version="1.0.0",
    docs_url="/docs",
    redoc_url="/redoc",
    openapi_url="/openapi.json",
    lifespan=lifespan
)

# Add CORS middleware
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Include cache cleanup router
app.include_router(cache_cleanup_router)

# Include document viewer router
app.include_router(document_viewer_router)

# ============================================================================
# AUTHENTICATION
# ============================================================================

# ============================================================================
# AUTHENTICATION ENDPOINTS (Public Access)
# ============================================================================

@app.post("/auth/register", response_model=dict)
async def register_user(
    user_data: UserRegister
):
    """–†–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏—è –Ω–æ–≤–æ–≥–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è"""
    try:
        from services.user_auth_service import user_auth_service
        
        result = user_auth_service.register_user(
            username=user_data.username,
            email=user_data.email,
            password=user_data.password,
            subscription_type=user_data.subscription_type,
            company_name=user_data.company_name
        )
        
        if result['success']:
            return {
                "message": "–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å —É—Å–ø–µ—à–Ω–æ –∑–∞—Ä–µ–≥–∏—Å—Ç—Ä–∏—Ä–æ–≤–∞–Ω",
                "user_id": result['user_id'],
                "username": result['username'],
                "subscription_type": result['subscription_type']
            }
        else:
            raise HTTPException(
                status_code=status.HTTP_400_BAD_REQUEST,
                detail=result['error']
            )
            
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Ä–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏–∏: {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail="–í–Ω—É—Ç—Ä–µ–Ω–Ω—è—è –æ—à–∏–±–∫–∞ —Å–µ—Ä–≤–µ—Ä–∞"
        )


@app.post("/auth/login", response_model=AuthResponse)
async def login_user(
    user_data: UserLogin
):
    """–ê—É—Ç–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ü–∏—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è"""
    try:
        from services.user_auth_service import user_auth_service
        
        result = user_auth_service.authenticate_user(user_data.username, user_data.password)
        
        if result['success']:
            return {
                "message": "–£—Å–ø–µ—à–Ω–∞—è –∞—É—Ç–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ü–∏—è",
                "access_token": result['access_token'],
                "refresh_token": result['refresh_token'],
                "user_info": result['user_info']
            }
        else:
            raise HTTPException(
                status_code=status.HTTP_401_UNAUTHORIZED,
                detail=result['error']
            )
            
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∞—É—Ç–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ü–∏–∏: {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail="–í–Ω—É—Ç—Ä–µ–Ω–Ω—è—è –æ—à–∏–±–∫–∞ —Å–µ—Ä–≤–µ—Ä–∞"
        )


@app.post("/auth/refresh")
async def refresh_token(
    refresh_token: str = Form(...)
):
    """–û–±–Ω–æ–≤–ª–µ–Ω–∏–µ access —Ç–æ–∫–µ–Ω–∞"""
    try:
        from services.user_auth_service import user_auth_service
        
        result = user_auth_service.refresh_access_token(refresh_token)
        
        if result['success']:
            return {
                "message": "–¢–æ–∫–µ–Ω –æ–±–Ω–æ–≤–ª–µ–Ω",
                "access_token": result['access_token']
            }
        else:
            raise HTTPException(
                status_code=status.HTTP_401_UNAUTHORIZED,
                detail=result['error']
            )
            
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–∏ —Ç–æ–∫–µ–Ω–∞: {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail="–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–∏ —Ç–æ–∫–µ–Ω–∞"
        )

@app.get("/auth/get-token", include_in_schema=False)
async def get_token_info(token: TokenValidation = Depends(get_current_token)):
    """–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ —Ç–æ–∫–µ–Ω–µ –∏ –ø—Ä–∞–≤–∞—Ö –¥–æ—Å—Ç—É–ø–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è"""
    # Import access control service
    from services.access_control_service import access_control_service
    
    # Get detailed access information
    detailed_access = access_control_service.get_detailed_access_info(token.access_level)
    access_summary = access_control_service.get_access_summary(token.access_level)
    
    return {
        "user_info": {
            "user_id": token.id,
            "access_level": token.access_level,
            "allowed_sections": token.allowed_sections
        },
        "detailed_access": detailed_access,
        "access_summary": access_summary,
        "message": "–î–µ—Ç–∞–ª—å–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –≤–∞—à–∏—Ö –ø—Ä–∞–≤–∞—Ö –¥–æ—Å—Ç—É–ø–∞",
        "note": "–ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ access_token –≤ –∑–∞–≥–æ–ª–æ–≤–∫–µ Authorization: Bearer <token> –¥–ª—è –¥–æ—Å—Ç—É–ø–∞ –∫ API"
    }


# ============================================================================
# PUBLIC API ENDPOINTS (Client Access)
# ============================================================================

@app.post("/documents/upload-smart", response_model=DocumentResponse)
async def upload_document_smart(
    file: UploadFile = File(...),
    title: Optional[str] = Form(None),
    description: Optional[str] = Form(None),
    section: str = Form(...),
    token: TokenValidation = Depends(get_current_token)
):
    """Smart document upload - automatically detects access level from token"""
    try:
        # Validate token
        if not token.is_valid:
            raise HTTPException(
                status_code=status.HTTP_401_UNAUTHORIZED,
                detail="–ù–µ–≤–µ—Ä–Ω—ã–π —Ç–æ–∫–µ–Ω"
            )
        
        # Auto-detect access level from token
        access_level = token.access_level
        
        # Import access control service
        from services.access_control_service import access_control_service
        
        # Validate access to section using detailed access control
        if not access_control_service.can_upload_to_section(token.access_level, section):
            raise HTTPException(
                status_code=status.HTTP_403_FORBIDDEN,
                detail=f"–î–æ—Å—Ç—É–ø –∫ –∑–∞–≥—Ä—É–∑–∫–µ –≤ —Ä–∞–∑–¥–µ–ª '{section}' –∑–∞–ø—Ä–µ—â–µ–Ω –¥–ª—è –≤–∞—à–µ–≥–æ —Ç–∏–ø–∞ –ø–æ–¥–ø–∏—Å–∫–∏ '{token.access_level}'. –û–±—Ä–∞—Ç–∏—Ç–µ—Å—å –∫ –∞–¥–º–∏–Ω–∏—Å—Ç—Ä–∞—Ç–æ—Ä—É –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –ø—Ä–∞–≤."
            )
        
        # Validate file type
        file_ext = os.path.splitext(file.filename)[1].lower()
        if file_ext not in settings.supported_formats:
            raise HTTPException(
                status_code=400,
                detail=f"–ù–µ–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–π —Ç–∏–ø —Ñ–∞–π–ª–∞: {file_ext}. –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–µ: {', '.join(settings.supported_formats)}"
            )
        
        # Validate file size
        if file.size > settings.max_file_size:
            raise HTTPException(
                status_code=400,
                detail=f"–§–∞–π–ª —Å–ª–∏—à–∫–æ–º –±–æ–ª—å—à–æ–π. –ú–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä: {settings.max_file_size / (1024*1024)}MB"
            )
        
        # Generate unique filename
        unique_filename = f"{uuid.uuid4()}{file_ext}"
        local_path = f"uploads/{unique_filename}"
        
        # Save file locally
        os.makedirs("uploads", exist_ok=True)
        with open(local_path, "wb") as f:
            content = await file.read()
            f.write(content)
        
        # Upload to Supabase
        supabase_path = f"{section}/{unique_filename}"
        public_url = supabase_service.upload_file(local_path, supabase_path)
        
        # Save document metadata to database
        db = get_db_session()
        try:
            document = Document(
                filename=unique_filename,
                original_filename=file.filename,
                file_path=supabase_path,
                file_size=file.size,
                file_type=file_ext,
                mime_type=file.content_type or "application/octet-stream",
                title=title or file.filename,
                description=description or f"–î–æ–∫—É–º–µ–Ω—Ç –∑–∞–≥—Ä—É–∂–µ–Ω –≤ —Ä–∞–∑–¥–µ–ª {section}",
                section=section,
                access_level=access_level,
                is_processed=False,
                has_images=False
            )
            
            db.add(document)
            db.commit()
            db.refresh(document)
            
            # Start async document processing
            # Pass the Supabase path (file_path) instead of local_path for processing
            asyncio.create_task(
                document_processor.process_document_async(
                    document.id, document.file_path, file_ext, section, access_level
                )
            )
            
            # Clean up local file
            os.remove(local_path)
            
            return DocumentResponse(
                id=document.id,
                filename=document.filename,
                original_filename=document.original_filename,
                file_path=document.file_path,
                file_size=document.file_size,
                file_type=document.file_type,
                mime_type=document.mime_type,
                title=document.title,
                description=document.description,
                section=document.section,
                access_level=document.access_level,
                is_processed=document.is_processed,
                processing_error=getattr(document, 'processing_error', None),
                has_images=document.has_images,
                text_content=getattr(document, 'text_content', None),
                extracted_metadata=getattr(document, 'extracted_metadata', None),
                uploaded_at=document.uploaded_at.isoformat() if document.uploaded_at else None,
                processed_at=document.processed_at.isoformat() if document.processed_at else None
            )
            
        except Exception as e:
            logger.error(f"–ó–∞–≥—Ä—É–∑–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞ –Ω–µ —É–¥–∞–ª–∞—Å—å: {e}")
            raise HTTPException(status_code=500, detail=str(e))
        finally:
            db.close()
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"–ó–∞–≥—Ä—É–∑–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞ –Ω–µ —É–¥–∞–ª–∞—Å—å: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/chat", response_model=ChatResponse)
async def chat_with_ai(
    request: ChatRequest,
    token: TokenValidation = Depends(get_current_token)
):
    """Enhanced chat with AI based on uploaded documents using section-based search and session context"""
    try:
        if not token.is_valid:
            raise HTTPException(
                status_code=status.HTTP_401_UNAUTHORIZED,
                detail="–ù–µ–≤–µ—Ä–Ω—ã–π —Ç–æ–∫–µ–Ω"
            )
        
        # –í–†–ï–ú–ï–ù–ù–û: –æ—Ç–∫–ª—é—á–∞–µ–º –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è –ø–æ —Ä–æ–ª—è–º –¥–ª—è /chat
        # –í—Å–µ —Ä–æ–ª–∏ –º–æ–≥—É—Ç –æ–±—Ä–∞—â–∞—Ç—å—Å—è –∫–æ –≤—Å–µ–º —Ä–∞–∑–¥–µ–ª–∞–º –¥–ª—è —á—Ç–µ–Ω–∏—è
        logger.info(f"üîì –í–†–ï–ú–ï–ù–ù–û: –æ—Ç–∫–ª—é—á–µ–Ω—ã –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è –ø–æ —Ä–æ–ª—è–º –¥–ª—è /chat")
        logger.info(f"üë§ –ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å: {token.access_level}")
        
        # –í–†–ï–ú–ï–ù–ù–û: –¥–∞–µ–º –¥–æ—Å—Ç—É–ø –∫–æ –≤—Å–µ–º —Ä–∞–∑–¥–µ–ª–∞–º –¥–ª—è —á—Ç–µ–Ω–∏—è
        user_sections = ["restaurant_ops", "procedures", "standards"]
        logger.info(f"‚úÖ –í–†–ï–ú–ï–ù–ù–û: –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å –∏–º–µ–µ—Ç –¥–æ—Å—Ç—É–ø –∫–æ –í–°–ï–ú —Ä–∞–∑–¥–µ–ª–∞–º: {user_sections}")
        
        # Generate or retrieve session ID
        session_id = request.session_id or f"session_{token.id}_{uuid.uuid4()}"
        
        # Get or create conversation
        conversation = await session_context_service.get_or_create_conversation(
            session_id=session_id,
            user_id=token.id,  # –ò—Å–ø–æ–ª—å–∑—É–µ–º id –∏–∑ —Ç–æ–∫–µ–Ω–∞ (–∫–æ—Ç–æ—Ä—ã–π —Å–æ–¥–µ—Ä–∂–∏—Ç user_id)
            initial_context=request.context
        )
        
        # –í–†–ï–ú–ï–ù–ù–û: —Ä–∞–∑—Ä–µ—à–∞–µ–º –¥–æ—Å—Ç—É–ø –∫ –ª—é–±–æ–π —Å–µ–∫—Ü–∏–∏ –¥–ª—è —á—Ç–µ–Ω–∏—è
        if request.section:
            if request.section in user_sections:
                await session_context_service.update_conversation_section(session_id, request.section)
                logger.info(f"‚úÖ –í–†–ï–ú–ï–ù–ù–û: –¥–æ—Å—Ç—É–ø –∫ —Å–µ–∫—Ü–∏–∏ '{request.section}' —Ä–∞–∑—Ä–µ—à–µ–Ω –¥–ª—è –≤—Å–µ—Ö —Ä–æ–ª–µ–π")
            else:
                logger.warning(f"‚ö†Ô∏è –ó–∞–ø—Ä–æ—à–µ–Ω–∞ –Ω–µ–∏–∑–≤–µ—Å—Ç–Ω–∞—è —Å–µ–∫—Ü–∏—è: {request.section}")
                # –í–†–ï–ú–ï–ù–ù–û: –≤—Å–µ —Ä–∞–≤–Ω–æ —Ä–∞–∑—Ä–µ—à–∞–µ–º –¥–æ—Å—Ç—É–ø
                await session_context_service.update_conversation_section(session_id, request.section)
                logger.info(f"‚úÖ –í–†–ï–ú–ï–ù–ù–û: –¥–æ—Å—Ç—É–ø –∫ —Å–µ–∫—Ü–∏–∏ '{request.section}' —Ä–∞–∑—Ä–µ—à–µ–Ω (–≤—Ä–µ–º–µ–Ω–Ω–æ–µ —Ä–µ—à–µ–Ω–∏–µ)")
        
        # Process images if provided
        image_analysis = {}
        enhanced_message = request.message
        
        # –õ–æ–≥–∏—Ä—É–µ–º –≤—Ö–æ–¥—è—â–∏–µ –¥–∞–Ω–Ω—ã–µ
        logger.info(f"üìù –í—Ö–æ–¥—è—â–µ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ: {request.message}")
        logger.info(f"üñºÔ∏è –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –≤ –∑–∞–ø—Ä–æ—Å–µ: {request.images}")
        logger.info(f"üîç –¢–∏–ø –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π: {type(request.images)}")
        if request.images:
            logger.info(f"üîç –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π: {len(request.images)}")
            for i, img in enumerate(request.images):
                logger.info(f"üîç –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ {i}: —Ç–∏–ø={img.image_type}, –æ–ø–∏—Å–∞–Ω–∏–µ={img.description}, –¥–∞–Ω–Ω—ã–µ={len(img.image_data)} —Å–∏–º–≤–æ–ª–æ–≤")
        
        if request.images:
            logger.info(f"üñºÔ∏è –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º {len(request.images)} –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π")
            logger.info(f"üîç –¢–∏–ø –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π: {type(request.images)}")
            logger.info(f"üîç –ü–µ—Ä–≤–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ: {request.images[0] if request.images else 'None'}")
            
            from services.image_processing_service import image_processing_service
            
            # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
            logger.info(f"üîç –ù–∞—á–∏–Ω–∞–µ–º –æ–±—Ä–∞–±–æ—Ç–∫—É –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π...")
            image_analysis = image_processing_service.process_chat_images(request.images)
            logger.info(f"üìä –†–µ–∑—É–ª—å—Ç–∞—Ç –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π: {image_analysis}")
            
            # –£–ª—É—á—à–∞–µ–º —Å–æ–æ–±—â–µ–Ω–∏–µ —Å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π –æ–± –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è—Ö
            enhanced_message = image_processing_service.enhance_chat_context(
                request.message, image_analysis
            )
            
            logger.info(f"‚úÖ –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π: {image_analysis.get('processed_images', 0)}")
            logger.info(f"üìù –ò–∑–≤–ª–µ—á–µ–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç: {image_analysis.get('extracted_text', [])}")
            logger.info(f"üîç –£–ª—É—á—à–µ–Ω–Ω–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ: {enhanced_message[:200]}...")
        else:
            logger.info("üìù –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –Ω–µ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω—ã")
        
        # Check if we should use existing context (for clarifying questions)
        # BUT only if no specific section is requested or if section matches
        use_existing_context = False
        existing_documents = []
        context_strategy = "new_search"
        
        if request.section:
            # If specific section is requested, NEVER reuse old context
            logger.info(f"üéØ –ó–∞–ø—Ä–æ—à–µ–Ω–∞ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–∞—è —Å–µ–∫—Ü–∏—è '{request.section}', –ø—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ –≤—ã–ø–æ–ª–Ω—è–µ–º –Ω–æ–≤—ã–π –ø–æ–∏—Å–∫")
            use_existing_context = False
            
            # Clear old context for this section to ensure fresh search
            try:
                await session_context_service.clear_document_context(session_id)
                logger.info(f"üßπ –û—á–∏—â–µ–Ω —Å—Ç–∞—Ä—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç –¥–ª—è —Å–µ–∫—Ü–∏–∏ '{request.section}'")
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –æ—á–∏—Å—Ç–∏—Ç—å –∫–æ–Ω—Ç–µ–∫—Å—Ç: {e}")
        else:
            # Check if we can reuse existing context
            use_existing_context, existing_documents, context_strategy = await session_context_service.should_use_existing_context(
                session_id, enhanced_message  # –ò—Å–ø–æ–ª—å–∑—É–µ–º —É–ª—É—á—à–µ–Ω–Ω–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ
            )
        
        search_results = []
        search_strategy = context_strategy
        
        if use_existing_context:
            if context_strategy == "context_reuse":
                logger.info(f"üîÑ –ò—Å–ø–æ–ª—å–∑—É–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π –∫–æ–Ω—Ç–µ–∫—Å—Ç –¥–ª—è —É—Ç–æ—á–Ω—è—é—â–µ–≥–æ –≤–æ–ø—Ä–æ—Å–∞ –≤ —Å–µ—Å—Å–∏–∏ {session_id}")
                # Convert existing documents to search result format
                for doc in existing_documents:
                    search_results.append({
                        'document_id': doc.get('document_id'),
                        'chunk_id': f"context_{doc.get('document_id')}",
                        'content': doc.get('content', ''),
                        'section': doc.get('section', ''),
                        'access_level': token.access_level,
                        'score': doc.get('score', 0.9),  # High score for context reuse
                        'metadata': {'context_reused': True, 'original_query': doc.get('query')}
                    })
            elif context_strategy == "hybrid_context":
                logger.info(f"üîÑ –ò—Å–ø–æ–ª—å–∑—É–µ–º –≥–∏–±—Ä–∏–¥–Ω—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –∫–æ–Ω—Ç–µ–∫—Å—Ç—É –≤ —Å–µ—Å—Å–∏–∏ {session_id}")
                # Use existing context as base, but also perform a focused search
                base_context = existing_documents
                
                # Perform focused search with existing context as guidance
                # BUT respect the requested section if specified
                target_section = request.section or session_context.get('current_section')
                if request.section:
                    logger.info(f"üéØ –ì–∏–±—Ä–∏–¥–Ω—ã–π –ø–æ–∏—Å–∫ —Å –ø—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ–π —Å–µ–∫—Ü–∏–µ–π: {target_section}")
                
                focused_results = await search_documents_internal(
                    query=request.message,
                    section=target_section,
                    access_level=token.access_level,
                    limit=6,  # Fewer results for focused search
                    score_threshold=0.6,  # Higher threshold for focused search
                    user_sections=user_sections,
                    strict_section_search=bool(request.section)  # Strict search if section specified
                )
                
                # Merge existing context with new focused results
                merged_context = await session_context_service.merge_contexts(
                    base_context, 
                    [{'document_id': r.document_id, 'content': r.content, 'section': r.section, 'score': r.score, 'query': request.message} for r in focused_results],
                    max_context_size=25
                )
                
                # Convert merged context to search result format
                for doc in merged_context:
                    search_results.append({
                        'document_id': doc.get('document_id'),
                        'chunk_id': f"hybrid_{doc.get('document_id')}",
                        'content': doc.get('content', ''),
                        'section': doc.get('section', ''),
                        'access_level': doc.get('access_level'),
                        'score': doc.get('score', 0.8),
                        'metadata': {'context_reused': True, 'hybrid_search': True, 'original_query': doc.get('query')}
                    })
        else:
            # Perform new search
            logger.info(f"üîç –í—ã–ø–æ–ª–Ω—è–µ–º –Ω–æ–≤—ã–π –ø–æ–∏—Å–∫ –¥–ª—è —Å–µ—Å—Å–∏–∏ {session_id}")
            
            # Determine search strategy based on request
            target_section = None
            logger.info(f"üîç DEBUG: request.section = {request.section}")
            logger.info(f"üîç DEBUG: user_sections = {user_sections}")
            logger.info(f"üîç DEBUG: request.section in user_sections = {request.section in user_sections if request.section else 'None'}")
            
            if request.section:
                # –í–†–ï–ú–ï–ù–ù–û: —Ä–∞–∑—Ä–µ—à–∞–µ–º –¥–æ—Å—Ç—É–ø –∫ –ª—é–±–æ–π —Å–µ–∫—Ü–∏–∏
                target_section = request.section
                search_strategy = "section_specific"
                logger.info(f"üéØ –í–†–ï–ú–ï–ù–ù–û: –ø–æ–∏—Å–∫ –ø–æ –∑–∞–ø—Ä–æ—à–µ–Ω–Ω–æ–π —Å–µ–∫—Ü–∏–∏: {target_section}")
            elif request.context and request.context in user_sections:
                target_section = request.context
                search_strategy = "section_specific"
                logger.info(f"üéØ –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø–æ–∏—Å–∫ –ø–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç—É: {target_section}")
            else:
                logger.info(f"üåê –ü–æ–∏—Å–∫ –ø–æ –≤—Å–µ–º –¥–æ—Å—Ç—É–ø–Ω—ã–º —Å–µ–∫—Ü–∏—è–º")
                # –í–†–ï–ú–ï–ù–ù–û: –Ω–µ –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–∞–µ–º –æ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω—ã—Ö —Å–µ–∫—Ü–∏—è—Ö
            
            # Perform enhanced search with smart filtering
            # –ï—Å–ª–∏ –µ—Å—Ç—å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è, –¥–æ–±–∞–≤–ª—è–µ–º –∏–∑–≤–ª–µ—á–µ–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç –∫ –∑–∞–ø—Ä–æ—Å—É
            search_query = enhanced_message if image_analysis.get('combined_text') else request.message
            
            # –í–†–ï–ú–ï–ù–ù–û: —É–ø—Ä–æ—â–∞–µ–º –ª–æ–≥–∏–∫—É –ø–æ–∏—Å–∫–∞
            logger.info(f"üîç DEBUG: request.section = {request.section}")
            logger.info(f"üîç DEBUG: target_section = {target_section}")
            logger.info(f"üîç DEBUG: user_sections = {user_sections}")
            
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Å—Ç—Ä–∞—Ç–µ–≥–∏—é –ø–æ–∏—Å–∫–∞
            if request.section:
                # –í–†–ï–ú–ï–ù–ù–û: –∏—Å–ø–æ–ª—å–∑—É–µ–º –±–æ–ª–µ–µ —Ç–æ—á–Ω—ã–π –ø–æ–∏—Å–∫ –¥–ª—è –ª—é–±–æ–π —Å–µ–∫—Ü–∏–∏
                strict_search = False  # –í—Å–µ–≥–¥–∞ –≥–∏–±–∫–∏–π –ø–æ–∏—Å–∫
                score_threshold = 0.5  # –°–Ω–∏–∂–µ–Ω–Ω—ã–π –ø–æ—Ä–æ–≥ –¥–ª—è –ª—É—á—à–µ–≥–æ –±–∞–ª–∞–Ω—Å–∞
                limit = 8  # –ú–µ–Ω—å—à–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤, –Ω–æ –ª—É—á—à–µ–≥–æ –∫–∞—á–µ—Å—Ç–≤–∞
                logger.info(f"üéØ –í–†–ï–ú–ï–ù–ù–û: –ø–æ–∏—Å–∫ –ø–æ —Å–µ–∫—Ü–∏–∏ '{request.section}' (—Å–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ä–µ–∂–∏–º, threshold={score_threshold})")
            else:
                strict_search = False  # –í—Å–µ–≥–¥–∞ –∏—Å–ø–æ–ª—å–∑—É–µ–º –≥–∏–±–∫–∏–π –ø–æ–∏—Å–∫
                score_threshold = 0.6  # –°–Ω–∏–∂–µ–Ω–Ω—ã–π –ø–æ—Ä–æ–≥ –¥–ª—è –æ–±—â–µ–≥–æ –ø–æ–∏—Å–∫–∞
                limit = 6  # –ú–µ–Ω—å—à–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤, –Ω–æ –ª—É—á—à–µ–≥–æ –∫–∞—á–µ—Å—Ç–≤–∞
                logger.info(f"üåê –û–±—â–∏–π –ø–æ–∏—Å–∫: –∏—Å–ø–æ–ª—å–∑—É–µ–º score_threshold={score_threshold}, limit={limit}")
            
            search_results = await search_documents_internal(
                query=search_query,
                section=target_section,
                access_level=token.access_level,
                limit=limit,
                score_threshold=score_threshold,
                user_sections=user_sections,
                strict_section_search=strict_search
            )
        
        if not search_results:
            # –í–º–µ—Å—Ç–æ –≤–æ–∑–≤—Ä–∞—Ç–∞ –ø—É—Å—Ç–æ–≥–æ –æ—Ç–≤–µ—Ç–∞, –∏—Å–ø–æ–ª—å–∑—É–µ–º –≥–∏–±—Ä–∏–¥–Ω—ã–π —Ä–µ–∂–∏–º
            logger.info(f"üîç –î–æ–∫—É–º–µ–Ω—Ç—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã, –ø–µ—Ä–µ–∫–ª—é—á–∞–µ–º—Å—è –Ω–∞ –≥–∏–±—Ä–∏–¥–Ω—ã–π —Ä–µ–∂–∏–º")
            
            # Add user message to conversation
            await session_context_service.add_message_to_conversation(
                conversation_id=conversation['id'],  # Use dictionary access
                role="user",
                content=request.message
            )
            
            # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –æ—Ç–≤–µ—Ç –∏—Å–ø–æ–ª—å–∑—É—è –æ–±—â–∏–µ –∑–Ω–∞–Ω–∏—è (–≥–∏–±—Ä–∏–¥–Ω—ã–π —Ä–µ–∂–∏–º)
            try:
                logger.info(f"ü§ñ –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –æ—Ç–≤–µ—Ç –≤ –≥–∏–±—Ä–∏–¥–Ω–æ–º —Ä–µ–∂–∏–º–µ (–±–µ–∑ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤)")
                ai_response = rag_service.generate_response(
                    query=request.message,
                    context_chunks=[],  # –ü—É—Å—Ç–æ–π –∫–æ–Ω—Ç–µ–∫—Å—Ç –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
                    conversation_history=[],
                    session_context={'document_context': [], 'messages': []}
                )
                
                return ChatResponse(
                    response=ai_response['response'],
                    session_id=session_id,
                    sources=[],
                    context_chunks_used=0,
                    timestamp=datetime.now().isoformat(),
                    follow_up_questions=ai_response.get('follow_up_questions', []),
                    response_strategy=ai_response.get('response_strategy', 'general_knowledge'),
                    question_analysis=ai_response.get('question_analysis', {})
                )
                
            except Exception as e:
                logger.error(f"‚ùå –ì–∏–±—Ä–∏–¥–Ω—ã–π —Ä–µ–∂–∏–º –Ω–µ —É–¥–∞–ª—Å—è: {e}")
                
                # Fallback –∫ –ø—Ä–æ—Å—Ç–æ–º—É –æ—Ç–≤–µ—Ç—É
                # –í–†–ï–ú–ï–ù–ù–û: –≤—Å–µ —Å–µ–∫—Ü–∏–∏ –¥–æ—Å—Ç—É–ø–Ω—ã, —É–ø—Ä–æ—â–∞–µ–º —Å–æ–æ–±—â–µ–Ω–∏—è
                if request.section:
                    response_message = f"–í —Å–µ–∫—Ü–∏–∏ '{request.section}' –Ω–µ –Ω–∞–π–¥–µ–Ω–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –¥–ª—è –æ—Ç–≤–µ—Ç–∞ –Ω–∞ –≤–∞—à –≤–æ–ø—Ä–æ—Å. –ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –ø–æ–ø—Ä–æ–±—É–π—Ç–µ –ø–µ—Ä–µ—Ñ—Ä–∞–∑–∏—Ä–æ–≤–∞—Ç—å –≤–æ–ø—Ä–æ—Å –∏–ª–∏ –æ–±—Ä–∞—Ç–∏—Ç–µ—Å—å –∫ –¥—Ä—É–≥–æ–π —Å–µ–∫—Ü–∏–∏."
                else:
                    response_message = "–Ø –Ω–µ —Å–º–æ–≥ –Ω–∞–π—Ç–∏ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –≤ –≤–∞—à–∏—Ö –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ö –¥–ª—è –æ—Ç–≤–µ—Ç–∞ –Ω–∞ —ç—Ç–æ—Ç –≤–æ–ø—Ä–æ—Å. –ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –ø–æ–ø—Ä–æ–±—É–π—Ç–µ –ø–µ—Ä–µ—Ñ—Ä–∞–∑–∏—Ä–æ–≤–∞—Ç—å —Å–≤–æ–π –≤–æ–ø—Ä–æ—Å –∏–ª–∏ –∑–∞–≥—Ä—É–∑–∏—Ç–µ –±–æ–ª–µ–µ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã."
                
                return ChatResponse(
                    response=response_message,
                    session_id=session_id,
                    sources=[],
                    context_chunks_used=0,
                    timestamp=datetime.now().isoformat(),
                    follow_up_questions=[]
                )
        
        # Get session context for enhanced RAG
        session_context = await session_context_service.get_conversation_context(session_id)
        logger.info(f"üìã –ü–æ–ª—É—á–µ–Ω –∫–æ–Ω—Ç–µ–∫—Å—Ç —Å–µ—Å—Å–∏–∏: {len(session_context.get('messages', []))} —Å–æ–æ–±—â–µ–Ω–∏–π, {len(session_context.get('document_context', []))} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤")
        
        # Debug: Log session context structure
        try:
            logger.info(f"üîç –°—Ç—Ä—É–∫—Ç—É—Ä–∞ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ —Å–µ—Å—Å–∏–∏: {json.dumps(session_context, default=str)[:500]}...")
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å —Å–µ—Ä–∏–∞–ª–∏–∑–æ–≤–∞—Ç—å –∫–æ–Ω—Ç–µ–∫—Å—Ç —Å–µ—Å—Å–∏–∏: {e}")
        
        # Build context from search results
        context_chunks = []
        for result in search_results:
            # Handle both SearchResult objects and dict objects
            if hasattr(result, 'content'):
                # SearchResult object
                context_chunks.append({
                    'content': result.content,
                    'document_id': result.document_id,
                    'chunk_id': result.chunk_id,
                    'section': result.section,
                    'access_level': result.access_level,
                    'score': result.score,
                    'metadata': result.metadata or {}
                })
            else:
                # Dict object (from context reuse)
                context_chunks.append({
                    'content': result.get('content', ''),
                    'document_id': result.get('document_id'),
                    'chunk_id': result.get('chunk_id'),
                    'section': result.get('section'),
                    'access_level': result.get('access_level'),
                    'score': result.get('score', 0.0),
                    'metadata': result.get('metadata', {})
                })
        
        logger.info(f"üîç –ü–æ—Å—Ç—Ä–æ–µ–Ω—ã {len(context_chunks)} —á–∞—Å—Ç–µ–π –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –¥–ª—è RAG (—Å—Ç—Ä–∞—Ç–µ–≥–∏—è: {search_strategy})")
        
        # Generate AI response using enhanced RAG with session context
        try:
            logger.info(f"ü§ñ –ù–∞—á–∏–Ω–∞–µ–º –≥–µ–Ω–µ—Ä–∞—Ü–∏—é –æ—Ç–≤–µ—Ç–∞ AI...")
            ai_response = rag_service.generate_response(
                query=request.message,
                context_chunks=context_chunks,
                conversation_history=session_context.get('messages', []),
                session_context=session_context
            )
            logger.info(f"‚úÖ –û—Ç–≤–µ—Ç AI —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω —É—Å–ø–µ—à–Ω–æ")
            
        except Exception as e:
            logger.error(f"‚ùå –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞ AI –Ω–µ —É–¥–∞–ª–∞—Å—å: {e}")
            logger.error(f"–ö–ª—é—á–∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ —Å–µ—Å—Å–∏–∏: {list(session_context.keys()) if session_context else 'None'}")
            logger.error(f"–¢–∏–ø –¥–æ–∫—É–º–µ–Ω—Ç–∞ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞: {type(session_context.get('document_context')) if session_context else 'None'}")
            if session_context and session_context.get('document_context'):
                logger.error(f"–î–ª–∏–Ω–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞: {len(session_context['document_context'])}")
                if session_context['document_context']:
                    logger.error(f"–ü–µ—Ä–≤—ã–π —Ç–∏–ø –¥–æ–∫—É–º–µ–Ω—Ç–∞: {type(session_context['document_context'][0])}")
            raise HTTPException(status_code=500, detail=f"–ù–µ —É–¥–∞–ª–æ—Å—å —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –æ—Ç–≤–µ—Ç AI: {str(e)}")
        
        # Add user message to conversation
        await session_context_service.add_message_to_conversation(
            conversation_id=conversation['id'],  # Use dictionary access
            role="user",
            content=request.message,
            search_context={
                'query': request.message,
                'results': context_chunks,
                'sections': [result.section if hasattr(result, 'section') else result.get('section') for result in search_results],
                'relevance_score': sum(result.score if hasattr(result, 'score') else result.get('score', 0) for result in search_results) / len(search_results) if search_results else 0,
                'source_chunks': [result.chunk_id if hasattr(result, 'chunk_id') else result.get('chunk_id') for result in search_results],
                'source_documents': [result.document_id if hasattr(result, 'document_id') else result.get('document_id') for result in search_results]
            }
        )
        
        # Add assistant response to conversation
        await session_context_service.add_message_to_conversation(
            conversation_id=conversation['id'],  # Use dictionary access
            role="assistant",
            content=ai_response['response']
        )
        
        # Extract source information
        sources = []
        for result in search_results:
            # Validate result before processing
            if not result:
                logger.warning(f"‚ö†Ô∏è –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –ø—É—Å—Ç–æ–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç")
                continue
                
            # Handle both SearchResult objects and dict objects
            if hasattr(result, 'content'):
                # SearchResult object
                if not hasattr(result, 'document_id') or not result.document_id:
                    logger.warning(f"‚ö†Ô∏è –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç –±–µ–∑ document_id: {result}")
                    continue
                    
                source_info = await get_document_info(result.document_id)
                document_name = get_better_document_name(source_info, result.document_id)
                
                # –î–æ–±–∞–≤–ª—è–µ–º –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏
                logger.info(f"üîç –°–æ–∑–¥–∞–µ–º –∏—Å—Ç–æ—á–Ω–∏–∫ –¥–ª—è –¥–æ–∫—É–º–µ–Ω—Ç–∞ {result.document_id}:")
                logger.info(f"   - source_info: {source_info}")
                logger.info(f"   - document_name: {document_name}")
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ document_name –Ω–µ —Ä–∞–≤–µ–Ω "string"
                if document_name == "string":
                    logger.error(f"‚ùå document_name —Ä–∞–≤–µ–Ω 'string' –¥–ª—è –¥–æ–∫—É–º–µ–Ω—Ç–∞ {result.document_id}")
                    # –ò—Å–ø–æ–ª—å–∑—É–µ–º fallback
                    document_name = f"–î–æ–∫—É–º–µ–Ω—Ç {result.document_id}"
                
                # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –ø—É—Å—Ç—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è
                if not document_name or document_name.strip() == "":
                    logger.error(f"‚ùå document_name –ø—É—Å—Ç–æ–π –¥–ª—è –¥–æ–∫—É–º–µ–Ω—Ç–∞ {result.document_id}")
                    document_name = f"–î–æ–∫—É–º–µ–Ω—Ç {result.document_id}"
                
                logger.info(f"‚úÖ –§–∏–Ω–∞–ª—å–Ω—ã–π document_name –¥–ª—è –¥–æ–∫—É–º–µ–Ω—Ç–∞ {result.document_id}: '{document_name}'")
                
                sources.append({
                    "document_id": result.document_id,
                    "chunk_id": result.chunk_id,
                    "section": result.section,
                    "access_level": result.access_level,
                    "document_title": document_name,
                    "document_name": document_name,
                    "chunk_type": result.chunk_type,
                    "page_number": result.page_number,
                    "section_name": result.section_name,
                    "metadata": result.metadata or {},
                    "context_reused": result.metadata.get('context_reused', False) if result.metadata else False
                })
            else:
                # Dict object (from context reuse)
                doc_id = result.get('document_id')
                if not doc_id:
                    logger.warning(f"‚ö†Ô∏è –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç –±–µ–∑ document_id: {result}")
                    continue
                    
                source_info = await get_document_info(doc_id)
                document_name = get_better_document_name(source_info, doc_id)
                
                # –î–æ–±–∞–≤–ª—è–µ–º –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏
                logger.info(f"üîç –°–æ–∑–¥–∞–µ–º –∏—Å—Ç–æ—á–Ω–∏–∫ –¥–ª—è –¥–æ–∫—É–º–µ–Ω—Ç–∞ {doc_id} (context reuse):")
                logger.info(f"   - source_info: {source_info}")
                logger.info(f"   - document_name: {document_name}")
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ document_name –Ω–µ —Ä–∞–≤–µ–Ω "string"
                if document_name == "string":
                    logger.error(f"‚ùå document_name —Ä–∞–≤–µ–Ω 'string' –¥–ª—è –¥–æ–∫—É–º–µ–Ω—Ç–∞ {doc_id} (context reuse)")
                    # –ò—Å–ø–æ–ª—å–∑—É–µ–º fallback
                    document_name = f"–î–æ–∫—É–º–µ–Ω—Ç {doc_id}"
                
                # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –ø—É—Å—Ç—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è
                if not document_name or document_name.strip() == "":
                    logger.error(f"‚ùå document_name –ø—É—Å—Ç–æ–π –¥–ª—è –¥–æ–∫—É–º–µ–Ω—Ç–∞ {doc_id} (context reuse)")
                    document_name = f"–î–æ–∫—É–º–µ–Ω—Ç {doc_id}"
                
                logger.info(f"‚úÖ –§–∏–Ω–∞–ª—å–Ω—ã–π document_name –¥–ª—è –¥–æ–∫—É–º–µ–Ω—Ç–∞ {doc_id} (context reuse): '{document_name}'")
                
                sources.append({
                    "document_id": doc_id,
                    "chunk_id": result.get('chunk_id'),
                    "section": result.get('section'),
                    "access_level": result.get('access_level'),
                    "document_title": document_name,
                    "document_name": document_name,
                    "chunk_type": "context_reuse",
                    "page_number": None,
                    "section_name": result.get('section'),
                    "metadata": result.get('metadata', {}),
                    "context_reused": result.get('metadata', {}).get('context_reused', False)
                })
        
        # –î–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏—è –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ –ø–æ document_id –∏ chunk_id
        unique_sources = []
        seen_combinations = set()
        
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –∏—Å—Ç–æ—á–Ω–∏–∫–∏ –ø–æ document_id –¥–ª—è –ª—É—á—à–µ–π –≥—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∏
        sorted_sources = sorted(sources, key=lambda x: (x.get('document_id', 0), x.get('chunk_id', 0)))
        
        for source in sorted_sources:
            # –°–æ–∑–¥–∞–µ–º —É–Ω–∏–∫–∞–ª—å–Ω—ã–π –∫–ª—é—á –¥–ª—è –∫–æ–º–±–∏–Ω–∞—Ü–∏–∏ document_id –∏ chunk_id
            unique_key = (source.get('document_id'), source.get('chunk_id'))
            
            if unique_key not in seen_combinations:
                seen_combinations.add(unique_key)
                unique_sources.append(source)
                logger.info(f"‚úÖ –î–æ–±–∞–≤–ª—è–µ–º —É–Ω–∏–∫–∞–ª—å–Ω—ã–π –∏—Å—Ç–æ—á–Ω–∏–∫: document_id={source.get('document_id')}, chunk_id={source.get('chunk_id')}")
            else:
                logger.info(f"üîÑ –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –¥—É–±–ª–∏–∫–∞—Ç: document_id={source.get('document_id')}, chunk_id={source.get('chunk_id')}")
        
        logger.info(f"üìä –ò—Å—Ç–æ—á–Ω–∏–∫–∏ –ø–æ—Å–ª–µ –¥–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏–∏: {len(unique_sources)} –∏–∑ {len(sources)}")
        
        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –≥—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∞ –ø–æ document_id –¥–ª—è –ª—É—á—à–µ–≥–æ –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è
        document_groups = {}
        for source in unique_sources:
            doc_id = source.get('document_id')
            if doc_id not in document_groups:
                document_groups[doc_id] = []
            document_groups[doc_id].append(source)
        
        logger.info(f"üìö –°–≥—Ä—É–ø–ø–∏—Ä–æ–≤–∞–Ω–æ –ø–æ {len(document_groups)} –¥–æ–∫—É–º–µ–Ω—Ç–∞–º: {list(document_groups.keys())}")
        for doc_id, doc_sources in document_groups.items():
            logger.info(f"   üìÑ –î–æ–∫—É–º–µ–Ω—Ç {doc_id}: {len(doc_sources)} —á–∞–Ω–∫–æ–≤")
        
        # –°–æ–∑–¥–∞–µ–º –∫–æ–º–ø–∞–∫—Ç–Ω—ã–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏ - –ø–æ –æ–¥–Ω–æ–º—É –Ω–∞ –¥–æ–∫—É–º–µ–Ω—Ç
        compact_sources = []
        for doc_id, doc_sources in document_groups.items():
            if doc_sources:
                # –ë–µ—Ä–µ–º –ø–µ—Ä–≤—ã–π –∏—Å—Ç–æ—á–Ω–∏–∫ –∫–∞–∫ –ø—Ä–µ–¥—Å—Ç–∞–≤–∏—Ç–µ–ª—å –¥–æ–∫—É–º–µ–Ω—Ç–∞
                representative_source = doc_sources[0].copy()
                
                # –î–æ–±–∞–≤–ª—è–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–µ —á–∞–Ω–∫–æ–≤
                representative_source['total_chunks'] = len(doc_sources)
                representative_source['chunk_ids'] = [s.get('chunk_id') for s in doc_sources]
                
                # –£–±–µ–∂–¥–∞–µ–º—Å—è, —á—Ç–æ —É –Ω–∞—Å –µ—Å—Ç—å mime_type –∏ file_type –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è —Å—Å—ã–ª–æ–∫
                if 'mime_type' not in representative_source or 'file_type' not in representative_source:
                    logger.warning(f"‚ö†Ô∏è –í –∏—Å—Ç–æ—á–Ω–∏–∫–µ –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç mime_type –∏–ª–∏ file_type: {representative_source}")
                    # –ü–æ–ø—Ä–æ–±—É–µ–º –ø–æ–ª—É—á–∏—Ç—å –∏–∑ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö
                    try:
                        doc_info = await get_document_info(doc_id)
                        if doc_info:
                            representative_source['mime_type'] = doc_info.get('mime_type', '')
                            representative_source['file_type'] = doc_info.get('file_type', '')
                            logger.info(f"‚úÖ –î–æ–±–∞–≤–ª–µ–Ω—ã mime_type –∏ file_type –¥–ª—è –¥–æ–∫—É–º–µ–Ω—Ç–∞ {doc_id}: {representative_source.get('mime_type')}, {representative_source.get('file_type')}")
                    except Exception as e:
                        logger.error(f"‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –¥–æ–∫—É–º–µ–Ω—Ç–µ {doc_id}: {e}")
                
                compact_sources.append(representative_source)
                logger.info(f"üìÑ –°–æ–∑–¥–∞–Ω –∫–æ–º–ø–∞–∫—Ç–Ω—ã–π –∏—Å—Ç–æ—á–Ω–∏–∫ –¥–ª—è –¥–æ–∫—É–º–µ–Ω—Ç–∞ {doc_id}: {len(doc_sources)} —á–∞–Ω–∫–æ–≤")
        
        logger.info(f"üìä –ö–æ–º–ø–∞–∫—Ç–Ω—ã–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏: {len(compact_sources)} –∏–∑ {len(unique_sources)}")
        
        # üîç –£–ú–ù–ê–Ø –§–ò–õ–¨–¢–†–ê–¶–ò–Ø –ò–°–¢–û–ß–ù–ò–ö–û–í
        # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Ç–æ–ª—å–∫–æ —Ç–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã, –∫–æ—Ç–æ—Ä—ã–µ –¥–µ–π—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ —Å–æ–¥–µ—Ä–∂–∞—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –¥–ª—è –æ—Ç–≤–µ—Ç–∞
        filtered_sources = await _filter_relevant_sources(compact_sources, ai_response['response'], context_chunks)
        logger.info(f"üéØ –û—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤: {len(filtered_sources)} –∏–∑ {len(compact_sources)}")
        
        # –§–æ—Ä–º–∞—Ç–∏—Ä—É–µ–º –æ—Ç–≤–µ—Ç —Å –æ—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω–Ω—ã–º–∏ –∏—Å—Ç–æ—á–Ω–∏–∫–∞–º–∏ —á–µ—Ä–µ–∑ source_linker
        formatted_response = source_linker.format_response_with_sources(ai_response['response'], filtered_sources)
        
        return ChatResponse(
            response=formatted_response,  # –ò—Å–ø–æ–ª—å–∑—É–µ–º –æ—Ç—Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –æ—Ç–≤–µ—Ç
            session_id=session_id,
            sources=filtered_sources,  # –ò—Å–ø–æ–ª—å–∑—É–µ–º –æ—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω–Ω—ã–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏
            context_chunks_used=len(context_chunks),
            timestamp=ai_response['timestamp'],
            follow_up_questions=ai_response.get('follow_up_questions', []),
            image_analysis=image_analysis if image_analysis else None
        )
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"–ß–∞—Ç –Ω–µ —É–¥–∞–ª—Å—è: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/search")
async def search_documents(
    request: DocumentSearchRequest,
    token: TokenValidation = Depends(get_current_token)
):
    """Search through uploaded documents"""
    try:
        if not token.is_valid:
            raise HTTPException(
                status_code=status.HTTP_401_UNAUTHORIZED,
                detail="–ù–µ–≤–µ—Ä–Ω—ã–π —Ç–æ–∫–µ–Ω"
            )
        
        # Import access control service
        from services.access_control_service import access_control_service
        
        # Get user's allowed sections using detailed access control
        user_sections = access_control_service.get_user_sections(token.access_level)
        
        # Perform search with optional strict section filtering
        strict_search = request.strict_section_search or bool(request.section)  # Use explicit flag or infer from section
        search_results = await search_documents_internal(
            query=request.query,
            section=request.section,
            access_level=request.access_level or token.access_level,
            limit=request.limit,
            score_threshold=request.score_threshold,
            user_sections=user_sections,
            strict_section_search=strict_search
        )
        
        # Format results
        formatted_results = []
        for result in search_results:
            # Validate result before processing
            if not result or not hasattr(result, 'document_id') or not result.document_id:
                logger.warning(f"‚ö†Ô∏è –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –Ω–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç: {result}")
                continue
                
            source_info = await get_document_info(result.document_id)
            formatted_results.append({
                "document_id": result.document_id,
                "chunk_id": result.chunk_id,
                "content": result.content,
                "score": result.score,
                "section": result.section,
                "access_level": result.access_level,
                "document_title": source_info.get("title", f"–î–æ–∫—É–º–µ–Ω—Ç {result.document_id}") if source_info else f"–î–æ–∫—É–º–µ–Ω—Ç {result.document_id}",
                "document_name": source_info.get("title", f"–î–æ–∫—É–º–µ–Ω—Ç {result.document_id}") if source_info else f"–î–æ–∫—É–º–µ–Ω—Ç {result.document_id}",
                "chunk_type": result.chunk_type,
                "page_number": result.page_number,
                "section_name": result.section_name,
                "metadata": result.metadata
            })
        
        return {
            "query": request.query,
            "results": formatted_results,
            "total_results": len(formatted_results),
            "search_parameters": {
                "section": request.section,
                "access_level": request.access_level or token.access_level,
                "limit": request.limit,
                "score_threshold": request.score_threshold
            }
        }
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"–ü–æ–∏—Å–∫ –Ω–µ —É–¥–∞–ª—Å—è: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/search/section")
async def search_documents_by_section_endpoint(
    request: DocumentSearchRequest,
    token: TokenValidation = Depends(get_current_token)
):
    """Search through documents in a specific section with enhanced precision"""
    try:
        if not token.is_valid:
            raise HTTPException(
                status_code=status.HTTP_401_UNAUTHORIZED,
                detail="–ù–µ–≤–µ—Ä–Ω—ã–π —Ç–æ–∫–µ–Ω"
            )
        
        if not request.section:
                raise HTTPException(
                status_code=status.HTTP_400_BAD_REQUEST,
                detail="–ü–∞—Ä–∞–º–µ—Ç—Ä —Ä–∞–∑–¥–µ–ª–∞ –æ–±—è–∑–∞—Ç–µ–ª–µ–Ω –¥–ª—è –ø–æ–∏—Å–∫–∞ –ø–æ —Ä–∞–∑–¥–µ–ª—É"
                )
            
        # Import access control service
        from services.access_control_service import access_control_service
        
        # Check if user has access to this section using detailed access control
        if not access_control_service.check_section_access(token.access_level, request.section, "read_only"):
            raise HTTPException(
                status_code=status.HTTP_403_FORBIDDEN,
                detail=f"–î–æ—Å—Ç—É–ø –∫ —Ä–∞–∑–¥–µ–ª—É '{request.section}' –∑–∞–ø—Ä–µ—â–µ–Ω –¥–ª—è –≤–∞—à–µ–≥–æ —Ç–∏–ø–∞ –ø–æ–¥–ø–∏—Å–∫–∏ '{token.access_level}'"
            )
        
        # Perform section-specific search with strict section filtering
        search_results = await search_documents_internal(
            query=request.query,
            section=request.section,
            access_level=request.access_level or token.access_level,
            limit=request.limit,
            score_threshold=request.score_threshold or 0.5,
            user_sections=[request.section],  # Only search in the specified section
            strict_section_search=True  # Enable strict section search
        )
        
        # Format results
        formatted_results = []
        for result in search_results:
            # Validate result before processing
            if not result or not hasattr(result, 'document_id') or not result.document_id:
                logger.warning(f"‚ö†Ô∏è –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –Ω–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç: {result}")
                continue
                
            source_info = await get_document_info(result.document_id)
            formatted_results.append({
                "document_id": result.document_id,
                "chunk_id": result.chunk_id,
                "content": result.content,
                "score": result.score,
                "section": result.section,
                "access_level": result.access_level,
                "document_title": source_info.get("title", f"–î–æ–∫—É–º–µ–Ω—Ç {result.document_id}") if source_info else f"–î–æ–∫—É–º–µ–Ω—Ç {result.document_id}",
                "document_name": source_info.get("title", f"–î–æ–∫—É–º–µ–Ω—Ç {result.document_id}") if source_info else f"–î–æ–∫—É–º–µ–Ω—Ç {result.document_id}",
                "chunk_type": result.chunk_type,
                "page_number": result.page_number,
                "section_name": result.section_name,
                "metadata": result.metadata
            })
        
        return {
            "query": request.query,
            "target_section": request.section,
            "results": formatted_results,
            "total_results": len(formatted_results),
            "search_parameters": {
                "section": request.section,
                "access_level": request.access_level or token.access_level,
                "limit": request.limit,
                "score_threshold": request.score_threshold or 0.5,
                "search_type": "section_specific"
            }
        }
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"–ü–æ–∏—Å–∫ –ø–æ —Ä–∞–∑–¥–µ–ª—É –Ω–µ —É–¥–∞–ª—Å—è: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/sessions/{session_id}/context")
async def get_session_context(
    session_id: str,
    token: TokenValidation = Depends(get_current_token)
):
    """Get the context and history of a specific session"""
    try:
        if not token.is_valid:
            raise HTTPException(
                status_code=status.HTTP_401_UNAUTHORIZED,
                detail="–ù–µ–≤–µ—Ä–Ω—ã–π —Ç–æ–∫–µ–Ω –¥–æ—Å—Ç—É–ø–∞"
            )
        
        # Get session context
        context = await session_context_service.get_conversation_context(session_id)
        
        if not context.get('conversation_id'):
            raise HTTPException(
                status_code=status.HTTP_404_NOT_FOUND,
                detail="–°–µ—Å—Å–∏—è –Ω–µ –Ω–∞–π–¥–µ–Ω–∞"
            )
        
        return {
            "session_id": session_id,
            "context": context
        }
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –∫–æ–Ω—Ç–µ–∫—Å—Ç —Å–µ—Å—Å–∏–∏: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@app.delete("/sessions/{session_id}")
async def delete_session(
    session_id: str,
    token: TokenValidation = Depends(get_current_token)
):
    """Delete a specific session and its context"""
    try:
        if not token.is_valid:
            raise HTTPException(
                status_code=status.HTTP_401_UNAUTHORIZED,
                detail="–ù–µ–≤–µ—Ä–Ω—ã–π —Ç–æ–∫–µ–Ω –¥–æ—Å—Ç—É–ø–∞"
            )
        
        # Get conversation
        db = get_db_session()
        try:
            conversation = db.query(Conversation).filter(
                Conversation.session_id == session_id
            ).first()
            
            if not conversation:
                raise HTTPException(
                    status_code=status.HTTP_404_NOT_FOUND,
                    detail="–°–µ—Å—Å–∏—è –Ω–µ –Ω–∞–π–¥–µ–Ω–∞"
                )
            
            # Delete conversation (cascade will delete messages)
            db.delete(conversation)
            db.commit()
            
            logger.info(f"üóëÔ∏è –£–¥–∞–ª–µ–Ω–∞ —Å–µ—Å—Å–∏—è: {session_id}")
            return {"message": f"–°–µ—Å—Å–∏—è {session_id} —É–¥–∞–ª–µ–Ω–∞ —É—Å–ø–µ—à–Ω–æ"}
            
        finally:
            db.close()
            
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"–ù–µ —É–¥–∞–ª–æ—Å—å —É–¥–∞–ª–∏—Ç—å —Å–µ—Å—Å–∏—é: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@app.get("/sessions")
async def list_user_sessions(
    token: TokenValidation = Depends(get_current_token)
):
    """List all sessions for the current user"""
    try:
        if not token.is_valid:
            raise HTTPException(
                status_code=status.HTTP_401_UNAUTHORIZED,
                detail="–ù–µ–≤–µ—Ä–Ω—ã–π —Ç–æ–∫–µ–Ω –¥–æ—Å—Ç—É–ø–∞"
            )
        
        # Get user's conversations
        db = get_db_session()
        try:
            conversations = db.query(Conversation).filter(
                Conversation.access_token_id == token.id
            ).order_by(Conversation.last_activity.desc()).all()
            
            sessions = []
            for conv in conversations:
                # Get message count
                message_count = db.query(ConversationMessage).filter(
                    ConversationMessage.conversation_id == conv.id
                ).count()
                
                sessions.append({
                    "session_id": conv.session_id,
                    "title": conv.title,
                    "current_section": conv.current_section,
                    "message_count": message_count,
                    "created_at": conv.created_at.isoformat(),
                    "last_activity": conv.last_activity.isoformat()
                })
            
            return {"sessions": sessions}
            
        finally:
            db.close()
            
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å —Å–ø–∏—Å–æ–∫ —Å–µ—Å—Å–∏–π: {e}")
        raise HTTPException(status_code=500, detail=str(e))

# ============================================================================
# INTERNAL/ADMIN ENDPOINTS (Hidden from Public Docs)
# ============================================================================

@app.get("/", include_in_schema=False)
async def root():
    """Root endpoint - API information"""
    return {
        "message": "üçΩÔ∏è Culinary RAG AI API",
        "version": "1.0.0",
        "description": "API –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å –∫—É–ª–∏–Ω–∞—Ä–Ω—ã–º–∏ –¥–æ–∫—É–º–µ–Ω—Ç–∞–º–∏ –∏ AI-—á–∞—Ç–æ–º",
        "docs": "/docs",
        "how_to_use": "–ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ /docs –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è API. –ü–æ–ª—É—á–∏—Ç–µ —Ç–æ–∫–µ–Ω —á–µ—Ä–µ–∑ /auth/register –∏–ª–∏ /auth/login"
    }



@app.get("/health", include_in_schema=False)
async def health_check():
    """Health check endpoint - hidden from public docs"""
    return {"status": "healthy", "timestamp": datetime.now().isoformat()}

@app.get("/documents/upload-options", include_in_schema=False)
async def get_upload_options(token: TokenValidation = Depends(get_current_token)):
    """Get upload options based on the user's token - hidden from public docs"""
    try:
        if not token.is_valid:
            raise HTTPException(
                status_code=status.HTTP_401_UNAUTHORIZED,
                detail="–ù–µ–≤–µ—Ä–Ω—ã–π —Ç–æ–∫–µ–Ω"
            )
        
        return {
            "access_level": token.access_level,
            "allowed_sections": token.allowed_sections,
            "supported_formats": settings.supported_formats,
            "max_file_size_mb": settings.max_file_size / (1024*1024),
            "message": f"–í–∞—à —Ç–æ–∫–µ–Ω —Ä–∞–∑—Ä–µ—à–∞–µ—Ç –¥–æ—Å—Ç—É–ø –∫ {token.access_level} —Å —Ä–∞–∑–¥–µ–ª–∞–º–∏: {', '.join(token.allowed_sections)}"
        }
        
    except Exception as e:
        logger.error(f"–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –∑–∞–≥—Ä—É–∑–∫–∏: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/documents/{document_id}/status", include_in_schema=False)
async def get_document_status(
    document_id: int,
    token: TokenValidation = Depends(get_current_token)
):
    """Get document processing status - hidden from public docs"""
    try:
        if not token.is_valid:
            raise HTTPException(
                status_code=status.HTTP_401_UNAUTHORIZED,
                detail="–ù–µ–≤–µ—Ä–Ω—ã–π —Ç–æ–∫–µ–Ω"
            )
        
        # Get document status
        status_info = await document_processor.get_processing_status(document_id)
        
        if 'error' in status_info:
            raise HTTPException(status_code=404, detail=status_info['error'])
        
        return status_info
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å —Å—Ç–∞—Ç—É—Å –¥–æ–∫—É–º–µ–Ω—Ç–∞: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/documents/{document_id}/reprocess", include_in_schema=False)
async def reprocess_document(
    document_id: int,
    token: TokenValidation = Depends(get_current_token)
):
    """Reprocess a document that failed previously - hidden from public docs"""
    try:
        if not token.is_valid:
            raise HTTPException(
                status_code=status.HTTP_401_UNAUTHORIZED,
                detail="–ù–µ–≤–µ—Ä–Ω—ã–π —Ç–æ–∫–µ–Ω"
            )
        
        # Check if user has access to this document
        db = get_db_session()
        try:
            document = db.query(Document).filter(Document.id == document_id).first()
            if not document:
                raise HTTPException(status_code=404, detail="–î–æ–∫—É–º–µ–Ω—Ç –Ω–µ –Ω–∞–π–¥–µ–Ω")
            
            # Import access control service
            from services.access_control_service import access_control_service
            
            # Check access control using detailed access control
            if not access_control_service.check_section_access(token.access_level, document.section, "read_only"):
                raise HTTPException(
                    status_code=status.HTTP_403_FORBIDDEN,
                    detail=f"–î–æ—Å—Ç—É–ø –∫ –¥–æ–∫—É–º–µ–Ω—Ç—É –≤ —Ä–∞–∑–¥–µ–ª–µ '{document.section}' –∑–∞–ø—Ä–µ—â–µ–Ω –¥–ª—è –≤–∞—à–µ–≥–æ —Ç–∏–ø–∞ –ø–æ–¥–ø–∏—Å–∫–∏ '{token.access_level}'"
                )
                
        finally:
            db.close()
        
        # Start reprocessing
        result = await document_processor.reprocess_document(document_id)
        
        if result['success']:
            return {"message": f"–ü–µ—Ä–µ–æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞ {document_id} –∑–∞–ø—É—â–µ–Ω–∞", "result": result}
        else:
            raise HTTPException(status_code=500, detail=f"–ü–µ—Ä–µ–æ–±—Ä–∞–±–æ—Ç–∫–∞ –Ω–µ —É–¥–∞–ª–∞—Å—å: {result.get('error', '–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–∞—è –æ—à–∏–±–∫–∞')}")
            
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"–ù–µ —É–¥–∞–ª–æ—Å—å –ø–µ—Ä–µ–æ–±—Ä–∞–±–æ—Ç–∞—Ç—å –¥–æ–∫—É–º–µ–Ω—Ç {document_id}: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.delete("/documents/clear-all")
async def clear_all_documents(
    token: TokenValidation = Depends(get_current_token)
):
    """Clear all documents for the current user (admin only)"""
    try:
        if not token.is_valid:
            raise HTTPException(
                status_code=status.HTTP_401_UNAUTHORIZED,
                detail="–ù–µ–≤–µ—Ä–Ω—ã–π —Ç–æ–∫–µ–Ω"
            )
        
        # Check if user is admin or has special permissions
        if token.access_level not in ['admin', 'restaurant_management']:
            raise HTTPException(
                status_code=status.HTTP_403_FORBIDDEN,
                detail="–ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –ø—Ä–∞–≤ –¥–ª—è —É–¥–∞–ª–µ–Ω–∏—è –≤—Å–µ—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤"
            )
        
        # Only proceed if user has proper permissions
        db = get_db_session()
        try:
            # Get all documents
            documents = db.query(Document).all()
            
            total_documents = len(documents) if documents else 0
            deleted_count = 0
            
            # –£–¥–∞–ª—è–µ–º –¥–æ–∫—É–º–µ–Ω—Ç—ã –∏–∑ –ë–î, –µ—Å–ª–∏ –æ–Ω–∏ –µ—Å—Ç—å
            if documents:
                for document in documents:
                    try:
                        # Delete document completely
                        result = await document_processor.delete_document_completely(document.id)
                        if result['success']:
                            deleted_count += 1
                        else:
                            logger.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å —É–¥–∞–ª–∏—Ç—å –¥–æ–∫—É–º–µ–Ω—Ç {document.id}: {result.get('error')}")
                    except Exception as e:
                        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —É–¥–∞–ª–µ–Ω–∏–∏ –¥–æ–∫—É–º–µ–Ω—Ç–∞ {document.id}: {e}")
                
                logger.info(f"üóëÔ∏è –ê–¥–º–∏–Ω–∏—Å—Ç—Ä–∞—Ç–æ—Ä {token.id} —É–¥–∞–ª–∏–ª {deleted_count}/{total_documents} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤")
            else:
                logger.info("‚ÑπÔ∏è –î–æ–∫—É–º–µ–Ω—Ç–æ–≤ –≤ –ë–î –Ω–µ –Ω–∞–π–¥–µ–Ω–æ, –ø–µ—Ä–µ—Ö–æ–¥–∏–º –∫ –æ—á–∏—Å—Ç–∫–µ Storage")
            
            # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –æ—á–∏—Å—Ç–∫–∞: —É–¥–∞–ª—è–µ–º –≤—Å–µ —Ñ–∞–π–ª—ã –∏–∑ Storage –Ω–∞–ø—Ä—è–º—É—é
            try:
                logger.info("üßπ –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –æ—á–∏—Å—Ç–∫–∞ Storage...")
                from services.supabase_service import supabase_service
                
                # –û—á–∏—â–∞–µ–º –≤—Å–µ –æ—Å–Ω–æ–≤–Ω—ã–µ —Ä–∞–∑–¥–µ–ª—ã
                sections_to_clean = ["procedures", "restaurant_ops", "standards", "recipes", "ingredients", "debug"]
                
                for section in sections_to_clean:
                    try:
                        section_files = supabase_service.list_files(section)
                        if section_files:
                            logger.info(f"üóëÔ∏è –ù–∞–π–¥–µ–Ω–æ {len(section_files)} —Ñ–∞–π–ª–æ–≤ –≤ {section} –¥–ª—è –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–π –æ—á–∏—Å—Ç–∫–∏")
                            
                            for file_info in section_files:
                                file_name = file_info.get('name', '')
                                if file_name and file_name != '.emptyFolderPlaceholder':
                                    try:
                                        file_path = f"{section}/{file_name}"
                                        if supabase_service.delete_file(file_path):
                                            logger.info(f"‚úÖ –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ —É–¥–∞–ª–µ–Ω —Ñ–∞–π–ª: {file_path}")
                                        else:
                                            logger.warning(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ —É–¥–∞–ª–∏—Ç—å: {file_path}")
                                    except Exception as e:
                                        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–º —É–¥–∞–ª–µ–Ω–∏–∏ {file_name}: {e}")
                        else:
                            logger.info(f"‚ÑπÔ∏è –†–∞–∑–¥–µ–ª {section} –ø—É—Å—Ç")
                            
                    except Exception as e:
                        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ—á–∏—Å—Ç–∫–µ —Ä–∞–∑–¥–µ–ª–∞ {section}: {e}")
                        
            except Exception as e:
                logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–π –æ—á–∏—Å—Ç–∫–µ Storage: {e}")
            
            return {
                "message": f"–£–¥–∞–ª–µ–Ω–æ {deleted_count} –∏–∑ {total_documents} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤" if documents else "Storage –æ—á–∏—â–µ–Ω",
                "total_documents": total_documents,
                "deleted_documents": deleted_count
            }
            
        finally:
            db.close()
            
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"–ù–µ —É–¥–∞–ª–æ—Å—å —É–¥–∞–ª–∏—Ç—å –≤—Å–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.delete("/documents/{document_id}")
async def delete_document(
    document_id: int,
    token: TokenValidation = Depends(get_current_token)
):
    """Delete a document completely - removes from storage, database, and vector store"""
    try:
        if not token.is_valid:
            raise HTTPException(
                status_code=status.HTTP_401_UNAUTHORIZED,
                detail="–ù–µ–≤–µ—Ä–Ω—ã–π —Ç–æ–∫–µ–Ω"
            )
        
        # Check if user has access to this document
        db = get_db_session()
        try:
            document = db.query(Document).filter(Document.id == document_id).first()
            if not document:
                raise HTTPException(status_code=404, detail="–î–æ–∫—É–º–µ–Ω—Ç –Ω–µ –Ω–∞–π–¥–µ–Ω")
            
            # Import access control service
            from services.access_control_service import access_control_service
            
            # Check access control using detailed access control
            if not access_control_service.can_delete_from_section(token.access_level, document.section):
                raise HTTPException(
                    status_code=status.HTTP_403_FORBIDDEN,
                    detail=f"–î–æ—Å—Ç—É–ø –∫ —É–¥–∞–ª–µ–Ω–∏—é –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –≤ —Ä–∞–∑–¥–µ–ª–µ '{document.section}' –∑–∞–ø—Ä–µ—â–µ–Ω –¥–ª—è –≤–∞—à–µ–≥–æ —Ç–∏–ø–∞ –ø–æ–¥–ø–∏—Å–∫–∏ '{token.access_level}'"
                )
                
            # Store document info for deletion
            document_info = {
                'id': document.id,
                'filename': document.filename,
                'original_filename': document.original_filename,
                'section': document.section
            }
                
        finally:
            db.close()
        
        # Delete document completely
        result = await document_processor.delete_document_completely(document_id)
        
        if result['success']:
            logger.info(f"üóëÔ∏è –î–æ–∫—É–º–µ–Ω—Ç {document_id} –ø–æ–ª–Ω–æ—Å—Ç—å—é —É–¥–∞–ª–µ–Ω –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º {token.id}")
            return {
                "message": f"–î–æ–∫—É–º–µ–Ω—Ç '{document_info['original_filename']}' —É–¥–∞–ª–µ–Ω —É—Å–ø–µ—à–Ω–æ",
                "deleted_document": document_info
            }
        else:
            raise HTTPException(
                status_code=500, 
                detail=f"–£–¥–∞–ª–µ–Ω–∏–µ –Ω–µ —É–¥–∞–ª–æ—Å—å: {result.get('error', '–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–∞—è –æ—à–∏–±–∫–∞')}"
            )
            
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"–ù–µ —É–¥–∞–ª–æ—Å—å —É–¥–∞–ª–∏—Ç—å –¥–æ–∫—É–º–µ–Ω—Ç {document_id}: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.delete("/conversations/clear-all")
async def clear_all_conversations(
    token: TokenValidation = Depends(get_current_token)
):
    """Clear all conversations and sessions for the current user"""
    try:
        if not token.is_valid:
            raise HTTPException(
                status_code=status.HTTP_401_UNAUTHORIZED,
                detail="–ù–µ–≤–µ—Ä–Ω—ã–π —Ç–æ–∫–µ–Ω"
            )
        
        db = get_db_session()
        try:
            # Get all conversations for this user
            conversations = db.query(Conversation).filter(Conversation.user_id == token.id).all()
            
            if not conversations:
                return {"message": "–£ –≤–∞—Å –Ω–µ—Ç –∞–∫—Ç–∏–≤–Ω—ã—Ö —á–∞—Ç–æ–≤ –¥–ª—è —É–¥–∞–ª–µ–Ω–∏—è"}
            
            total_conversations = len(conversations)
            total_messages = 0
            
            for conversation in conversations:
                # Delete all messages in this conversation
                messages_deleted = db.query(ConversationMessage).filter(
                    ConversationMessage.conversation_id == conversation.id
                ).delete()
                total_messages += messages_deleted
                
                # Delete the conversation
                db.delete(conversation)
            
            db.commit()
            
            logger.info(f"üóëÔ∏è –ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å {token.id} —É–¥–∞–ª–∏–ª {total_conversations} —á–∞—Ç–æ–≤ –∏ {total_messages} —Å–æ–æ–±—â–µ–Ω–∏–π")
            
            return {
                "message": f"–£–¥–∞–ª–µ–Ω–æ {total_conversations} —á–∞—Ç–æ–≤ –∏ {total_messages} —Å–æ–æ–±—â–µ–Ω–∏–π",
                "deleted_conversations": total_conversations,
                "deleted_messages": total_messages
            }
            
        finally:
            db.close()
            
    except Exception as e:
        logger.error(f"–ù–µ —É–¥–∞–ª–æ—Å—å —É–¥–∞–ª–∏—Ç—å —á–∞—Ç—ã: {e}")
        raise HTTPException(status_code=500, detail=str(e))

# ============================================================================
# ADMIN ENDPOINTS (Completely Hidden from Public)
# ============================================================================

# Admin token validation moved to auth_dependencies.py

@app.post("/admin/tokens", response_model=AccessTokenResponse, include_in_schema=False)
async def create_access_token(
    request: AccessTokenCreate,
    admin_token: str = Depends(get_admin_token)
):
    """Create access token - admin only, hidden from public docs"""
    try:
        db = get_db_session()
        try:
            # Generate token
            actual_token, token_hash = admin_service._generate_token_hash()
            
            # Create access token
            access_token = AccessToken(
                token_hash=token_hash,
                name=request.name,
                description=request.description,
                access_level=request.access_level,
                allowed_sections=request.allowed_sections,
                rate_limit_per_hour=request.rate_limit_per_hour,
                expires_at=datetime.fromisoformat(request.expires_at) if request.expires_at else None
            )
            
            db.add(access_token)
            db.commit()
            db.refresh(access_token)
            
            return AccessTokenResponse(
                id=access_token.id,
                token_hash=actual_token,  # Return the actual token, not the hash
                name=access_token.name,
                description=access_token.description,
                access_level=access_token.access_level,
                allowed_sections=access_token.allowed_sections,
                is_active=access_token.is_active,
                rate_limit_per_hour=access_token.rate_limit_per_hour,
                current_usage=access_token.current_usage,
                created_at=access_token.created_at.isoformat() if access_token.created_at else None,
                expires_at=access_token.expires_at.isoformat() if access_token.expires_at else None
            )
            
        finally:
            db.close()
            
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"–ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ–∑–¥–∞—Ç—å —Ç–æ–∫–µ–Ω –¥–æ—Å—Ç—É–ø–∞: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/admin/tokens", response_model=List[AccessTokenResponse], include_in_schema=False)
async def list_access_tokens(admin_token: str = Depends(get_admin_token)):
    """List access tokens - admin only, hidden from public docs"""
    try:
        db = get_db_session()
        try:
            tokens = db.query(AccessToken).all()
            return [
                AccessTokenResponse(
                id=token.id,
                    token_hash=token.token_hash,
                name=token.name,
                description=token.description,
                access_level=token.access_level,
                allowed_sections=token.allowed_sections,
                is_active=token.is_active,
                rate_limit_per_hour=token.rate_limit_per_hour,
                current_usage=token.current_usage,
                    created_at=token.created_at.isoformat() if token.created_at else None,
                    expires_at=token.expires_at.isoformat() if token.expires_at else None
                )
                for token in tokens
            ]
        finally:
            db.close()
    except Exception as e:
        logger.error(f"–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å —Å–ø–∏—Å–æ–∫ —Ç–æ–∫–µ–Ω–æ–≤ –¥–æ—Å—Ç—É–ø–∞: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/admin/analytics", include_in_schema=False)
async def get_admin_analytics(admin_token: str = Depends(get_admin_token)):
    """Get system analytics (admin only)"""
    try:
        # Get basic statistics
        db = get_db_session()
        try:
            total_documents = db.query(Document).count()
            processed_documents = db.query(Document).filter(Document.is_processed == True).count()
            error_documents = db.query(Document).filter(Document.processing_error.isnot(None)).count()
            
            # Get recent uploads
            recent_documents = db.query(Document).order_by(Document.uploaded_at.desc()).limit(5).all()
            
            analytics = {
                "total_documents": total_documents,
                "processed_documents": processed_documents,
                "error_documents": error_documents,
                "processing_success_rate": (processed_documents / total_documents * 100) if total_documents > 0 else 0,
                "recent_documents": [
                    {
                        "id": doc.id,
                        "title": doc.title,
                        "section": doc.section,
                        "uploaded_at": doc.uploaded_at.isoformat() if doc.uploaded_at else None,
                        "is_processed": doc.is_processed,
                        "error": doc.processing_error
                    }
                    for doc in recent_documents
                ]
            }
            
            return analytics
            
        finally:
            db.close()
        
    except Exception as e:
        logger.error(f"–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –∞–Ω–∞–ª–∏—Ç–∏–∫—É: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/admin/documents/{document_id}/reprocess", include_in_schema=False)
async def reprocess_document_admin(
    document_id: int,
    admin_token: str = Depends(get_admin_token)
):
    """Reprocess a document (admin only)"""
    try:
        # Get document info
        db = get_db_session()
        try:
            document = db.query(Document).filter(Document.id == document_id).first()
            if not document:
                raise HTTPException(status_code=404, detail="–î–æ–∫—É–º–µ–Ω—Ç –Ω–µ –Ω–∞–π–¥–µ–Ω")
        finally:
            db.close()
        
        # Reprocess document
        result = await document_processor.reprocess_document(document_id)
        
        if result['success']:
            return {"message": f"–î–æ–∫—É–º–µ–Ω—Ç {document_id} –ø–µ—Ä–µ–æ–±—Ä–∞–±–æ—Ç–∞–Ω —É—Å–ø–µ—à–Ω–æ", "result": result}
        else:
            raise HTTPException(status_code=500, detail=f"–ü–µ—Ä–µ–æ–±—Ä–∞–±–æ—Ç–∫–∞ –Ω–µ —É–¥–∞–ª–∞—Å—å: {result.get('error', '–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–∞—è –æ—à–∏–±–∫–∞')}")
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"–ù–µ —É–¥–∞–ª–æ—Å—å –ø–µ—Ä–µ–æ–±—Ä–∞–±–æ—Ç–∞—Ç—å –¥–æ–∫—É–º–µ–Ω—Ç {document_id}: {e}")
        raise HTTPException(status_code=500, detail=str(e))

# ============================================================================
# INTERNAL HELPER FUNCTIONS
# ============================================================================

async def search_documents_internal(
    query: str,
    section: Optional[str] = None,
    access_level: Optional[str] = None,
    limit: int = 10,
    score_threshold: float = 0.7,
    user_sections: Optional[List[str]] = None,
    strict_section_search: bool = False
) -> List[DocumentSearchResult]:
    """Enhanced internal search function with section-based logic and intelligent fallback"""
    try:
        logger.info(f"üîç –ò—â–µ–º: '{query}' –≤ —Ä–∞–∑–¥–µ–ª–µ: {section}, —É—Ä–æ–≤–µ–Ω—å –¥–æ—Å—Ç—É–ø–∞: {access_level}")
        logger.info(f"üîí –°—Ç—Ä–æ–≥–∏–π –ø–æ–∏—Å–∫: {strict_section_search}")
        logger.info(f"üéØ –ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–µ —Å–µ–∫—Ü–∏–∏: {user_sections}")
        
        # Step 1: Section-specific search (if section is specified and user has access)
        section_results = []
        if section:
            logger.info(f"üéØ –ó–∞–ø—Ä–æ—à–µ–Ω–∞ —Å–µ–∫—Ü–∏—è: {section}")
            logger.info(f"üéØ –î–æ—Å—Ç—É–ø–Ω—ã–µ —Å–µ–∫—Ü–∏–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è: {user_sections}")
            
            if section in (user_sections or []):
                logger.info(f"‚úÖ –ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å –∏–º–µ–µ—Ç –¥–æ—Å—Ç—É–ø –∫ —Å–µ–∫—Ü–∏–∏ {section}, –≤—ã–ø–æ–ª–Ω—è–µ–º –ø–æ–∏—Å–∫")
                logger.info(f"üîí –°—Ç—Ä–æ–≥–∏–π –ø–æ–∏—Å–∫: {strict_section_search}")
                section_results = await _perform_section_search(
                    query=query,
                    target_section=section,
                    access_level=access_level,
                    limit=limit,
                    score_threshold=score_threshold
                )
                logger.info(f"üîç –ü–æ–∏—Å–∫ –ø–æ —Å–µ–∫—Ü–∏–∏ {section} –≤–µ—Ä–Ω—É–ª {len(section_results)} —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤")
            else:
                logger.warning(f"‚ùå –ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å –ù–ï –∏–º–µ–µ—Ç –¥–æ—Å—Ç—É–ø–∞ –∫ —Å–µ–∫—Ü–∏–∏ {section}")
                if strict_section_search:
                    logger.info(f"üîí –°—Ç—Ä–æ–≥–∏–π –ø–æ–∏—Å–∫: –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –ø—É—Å—Ç–æ–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç –¥–ª—è –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–æ–π —Å–µ–∫—Ü–∏–∏ {section}")
                    return []
                else:
                    logger.info(f"üîÑ –ü–µ—Ä–µ–∫–ª—é—á–∞–µ–º—Å—è –Ω–∞ –ø–æ–∏—Å–∫ –ø–æ –≤—Å–µ–º –¥–æ—Å—Ç—É–ø–Ω—ã–º —Ä–∞–∑–¥–µ–ª–∞–º")
                    section = None  # Reset section to trigger fallback search
                    section_results = []
        
        # Check if the specified section is empty or has no relevant results
        if section and not section_results:
            logger.warning(f"‚ö†Ô∏è –†–∞–∑–¥–µ–ª '{section}' –ø—É—Å—Ç –∏–ª–∏ –Ω–µ —Å–æ–¥–µ—Ä–∂–∏—Ç —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤")
            if strict_section_search:
                logger.info(f"üîí –°—Ç—Ä–æ–≥–∏–π –ø–æ–∏—Å–∫: –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –ø—É—Å—Ç–æ–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç –¥–ª—è –ø—É—Å—Ç–æ–≥–æ —Ä–∞–∑–¥–µ–ª–∞ '{section}'")
                return []
            else:
                logger.info(f"üîÑ –ü–µ—Ä–µ–∫–ª—é—á–∞–µ–º—Å—è –Ω–∞ –ø–æ–∏—Å–∫ –ø–æ –≤—Å–µ–º –¥–æ—Å—Ç—É–ø–Ω—ã–º —Ä–∞–∑–¥–µ–ª–∞–º")
                section = None  # Reset section to trigger fallback search
        elif section and section_results:
            # If strict section search is enabled, return only section results
            if strict_section_search:
                logger.info(f"üîí –°—Ç—Ä–æ–≥–∏–π –ø–æ–∏—Å–∫ –ø–æ —Ä–∞–∑–¥–µ–ª—É: –≤–æ–∑–≤—Ä–∞—â–∞–µ–º —Ç–æ–ª—å–∫–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –∏–∑ {section}")
                return section_results[:limit]
            
            # If we found good results in the specified section, return them
            if any(result.score > score_threshold * 0.8 for result in section_results):
                logger.info(f"‚úÖ –ù–∞–π–¥–µ–Ω–æ {len(section_results)} —Ö–æ—Ä–æ—à–∏—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –≤ —Ä–∞–∑–¥–µ–ª–µ {section}")
                return section_results[:limit]
            
            logger.info(f"‚ö†Ô∏è –ü–æ–∏—Å–∫ –ø–æ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–º—É —Ä–∞–∑–¥–µ–ª—É –≤–µ—Ä–Ω—É–ª {len(section_results)} —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤, –ø—ã—Ç–∞–µ–º—Å—è –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–π –ø–æ–¥—Ö–æ–¥")
        
        # Step 2: Fallback search across all user's allowed sections (only if not strict)
        if not strict_section_search and user_sections:
            logger.info(f"üåê –í—ã–ø–æ–ª–Ω—è–µ–º –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–π –ø–æ–∏—Å–∫ –ø–æ –≤—Å–µ–º —Ä–∞–∑—Ä–µ—à–µ–Ω–Ω—ã–º —Ä–∞–∑–¥–µ–ª–∞–º: {user_sections}")
            fallback_results = await _perform_fallback_search(
                query=query,
                access_level=access_level,
                limit=limit * 2,  # Get more results for fallback
                score_threshold=score_threshold * 0.6,  # Lower threshold for fallback
                user_sections=user_sections
            )
        else:
            fallback_results = []
            if strict_section_search:
                logger.info(f"üîí –°—Ç—Ä–æ–≥–∏–π –ø–æ–∏—Å–∫: –ø—Ä–æ–ø—É—Å–∫–∞–µ–º fallback –ø–æ–∏—Å–∫ –ø–æ –≤—Å–µ–º —Å–µ–∫—Ü–∏—è–º")
                # –ï—Å–ª–∏ —Å—Ç—Ä–æ–≥–∏–π –ø–æ–∏—Å–∫ –∏ —Å–µ–∫—Ü–∏—è –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –ø—É—Å—Ç–æ–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç
                if section and section not in (user_sections or []):
                    logger.warning(f"üîí –°—Ç—Ä–æ–≥–∏–π –ø–æ–∏—Å–∫: —Å–µ–∫—Ü–∏—è '{section}' –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞ –¥–ª—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è")
                    return []
        
        # Step 3: Combine and rank results
        all_results = []
        if section_results:
            all_results.extend(section_results)
        
        # Only add fallback results if not strict section search
        if not strict_section_search:
            all_results.extend(fallback_results)
        else:
            logger.info(f"üîí –°—Ç—Ä–æ–≥–∏–π –ø–æ–∏—Å–∫: –∏—Å–ø–æ–ª—å–∑—É–µ–º —Ç–æ–ª—å–∫–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –∏–∑ —É–∫–∞–∑–∞–Ω–Ω–æ–π —Å–µ–∫—Ü–∏–∏")
        
        # Remove duplicates and sort by score
        unique_results = {}
        for result in all_results:
            key = (result.document_id, result.chunk_id)
            if key not in unique_results or result.score > unique_results[key].score:
                unique_results[key] = result
        
        # Sort by score and return top results
        final_results = sorted(unique_results.values(), key=lambda x: x.score, reverse=True)
        logger.info(f"üéâ –§–∏–Ω–∞–ª—å–Ω—ã–π –ø–æ–∏—Å–∫ –≤–µ—Ä–Ω—É–ª {len(final_results)} —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤")
        
        return final_results[:limit]
        
    except Exception as e:
        logger.error(f"‚ùå –£–ª—É—á—à–µ–Ω–Ω—ã–π –ø–æ–∏—Å–∫ –Ω–µ —É–¥–∞–ª—Å—è: {e}")
        return []

async def _perform_section_search(
    query: str,
    target_section: str,
    access_level: Optional[str] = None,
    limit: int = 10,
    score_threshold: float = 0.7
) -> List[DocumentSearchResult]:
    """Perform focused search within a specific section"""
    try:
        logger.info(f"üéØ –ü–æ–∏—Å–∫ –ø–æ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–º—É —Ä–∞–∑–¥–µ–ª—É –≤ '{target_section}' –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞: '{query}'")
        
        # Get embeddings for the query
        query_embedding = await embedding_service.get_embeddings_async(query)
        
        # Build strict filters for section-specific search
        filters = {
            'section': target_section
        }
        if access_level:
            filters['access_level'] = access_level
        
        # Use higher score threshold for section-specific search to ensure quality
        adjusted_score_threshold = max(score_threshold, 0.6)
        
        # Search in vector database with section-specific focus
        raw_results = vector_service.search_similar(
            query_embedding=query_embedding,
            limit=limit * 2,  # Get more results initially for better filtering
            score_threshold=adjusted_score_threshold,
            filters=filters
        )
        
        # Convert and filter results
        search_results = []
        for result in raw_results:
            search_result = DocumentSearchResult(
                document_id=result.get('document_id'),
                chunk_id=result.get('chunk_id'),
                content=result.get('content', ''),
                score=result.get('score', 0.0),
                section=result.get('section', ''),
                access_level=result.get('access_level', ''),
                chunk_type=result.get('chunk_type'),
                page_number=result.get('page_number'),
                section_name=result.get('section_name'),
                metadata=result.get('metadata', {})
            )
            search_results.append(search_result)
        
        # Sort by score and limit results
        search_results.sort(key=lambda x: x.score, reverse=True)
        logger.info(f"‚úÖ –ü–æ–∏—Å–∫ –ø–æ —Ä–∞–∑–¥–µ–ª—É –Ω–∞—à–µ–ª {len(search_results)} —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –≤ '{target_section}'")
        return search_results[:limit]
        
    except Exception as e:
        logger.error(f"‚ùå –ü–æ–∏—Å–∫ –ø–æ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–º—É —Ä–∞–∑–¥–µ–ª—É –Ω–µ —É–¥–∞–ª—Å—è: {e}")
        return []

async def _perform_fallback_search(
    query: str,
    access_level: Optional[str] = None,
    limit: int = 20,
    score_threshold: float = 0.5,
    user_sections: Optional[List[str]] = None
) -> List[DocumentSearchResult]:
    """Perform fallback search across all allowed sections"""
    try:
        logger.info(f"üåê –ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–π –ø–æ–∏—Å–∫ –ø–æ —Ä–∞–∑–¥–µ–ª–∞–º: {user_sections}")
        
        # Get embeddings for the query
        query_embedding = await embedding_service.get_embeddings_async(query)
        
        # Build filters for fallback search
        filters = {}
        if access_level:
            filters['access_level'] = access_level
        
        # If user has specific sections, search in each one
        if user_sections:
            logger.info(f"üîç –ò—â–µ–º –≤ {len(user_sections)} –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏—Ö —Ä–∞–∑–¥–µ–ª–∞—Ö: {user_sections}")
            all_results = []
            
            for section in user_sections:
                section_filters = filters.copy()
                section_filters['section'] = section
                
                try:
                    logger.info(f"üîç –ò—â–µ–º –≤ —Ä–∞–∑–¥–µ–ª–µ: {section}")
                    section_results = vector_service.search_similar(
                        query_embedding=query_embedding,
                        limit=limit // len(user_sections),  # Distribute limit across sections
                        score_threshold=score_threshold,
                        filters=section_filters
                    )
                    
                    logger.info(f"üìä –†–∞–∑–¥–µ–ª '{section}' –≤–µ—Ä–Ω—É–ª {len(section_results)} —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤")
                    
                    # Convert results
                    for result in section_results:
                        search_result = DocumentSearchResult(
                            document_id=result.get('document_id'),
                            chunk_id=result.get('chunk_id'),
                            content=result.get('content', ''),
                            score=result.get('score', 0.0),
                            section=result.get('section', ''),
                            access_level=result.get('access_level', ''),
                            chunk_type=result.get('chunk_type'),
                            page_number=result.get('page_number'),
                            section_name=result.get('section_name'),
                            metadata=result.get('metadata', {})
                        )
                        all_results.append(search_result)
                        
                except Exception as e:
                    logger.warning(f"‚ö†Ô∏è –ü–æ–∏—Å–∫ –≤ —Ä–∞–∑–¥–µ–ª–µ {section} –Ω–µ —É–¥–∞–ª—Å—è: {e}")
                    continue
            
            # Sort by score and return top results
            all_results.sort(key=lambda x: x.score, reverse=True)
            logger.info(f"‚úÖ –ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–π –ø–æ–∏—Å–∫ –Ω–∞—à–µ–ª {len(all_results)} –æ–±—â–∏—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –ø–æ –≤—Å–µ–º —Ä–∞–∑–¥–µ–ª–∞–º")
            return all_results[:limit]
        
        else:
            # No specific sections, search globally
            logger.info("üåç –í—ã–ø–æ–ª–Ω—è–µ–º –≥–ª–æ–±–∞–ª—å–Ω—ã–π –ø–æ–∏—Å–∫ (–±–µ–∑ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–π –ø–æ —Ä–∞–∑–¥–µ–ª–∞–º)")
            raw_results = vector_service.search_similar(
                query_embedding=query_embedding,
                limit=limit,
                score_threshold=score_threshold,
                filters=filters
            )
            
            # Convert results
            search_results = []
            for result in raw_results:
                search_result = DocumentSearchResult(
                    document_id=result.get('document_id'),
                    chunk_id=result.get('chunk_id'),
                    content=result.get('content', ''),
                    score=result.get('score', 0.0),
                    section=result.get('section', ''),
                    access_level=result.get('access_level', ''),
                    chunk_type=result.get('chunk_type'),
                    page_number=result.get('page_number'),
                    section_name=result.get('section_name'),
                    metadata=result.get('metadata', {})
                )
                search_results.append(search_result)
            
            logger.info(f"‚úÖ –ì–ª–æ–±–∞–ª—å–Ω—ã–π –ø–æ–∏—Å–∫ –Ω–∞—à–µ–ª {len(search_results)} —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤")
            return search_results
        
    except Exception as e:
        logger.error(f"‚ùå –ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–π –ø–æ–∏—Å–∫ –Ω–µ —É–¥–∞–ª—Å—è: {e}")
        return []

async def search_documents_by_section(
    query: str,
    target_section: str,
    access_level: Optional[str] = None,
    limit: int = 10,
    score_threshold: float = 0.5
) -> List[DocumentSearchResult]:
    """Enhanced search function specifically for section-based queries"""
    try:
        # Get embeddings for the query
        query_embedding = await embedding_service.get_embeddings_async(query)
        
        # Build strict filters for section-specific search
        filters = {
            'section': target_section
        }
        if access_level:
            filters['access_level'] = access_level
        
        # Use higher score threshold for section-specific search to ensure quality
        adjusted_score_threshold = max(score_threshold, 0.6)
        
        # Search in vector database with section-specific focus
        raw_results = vector_service.search_similar(
            query_embedding=query_embedding,
            limit=limit * 2,  # Get more results initially for better filtering
            score_threshold=adjusted_score_threshold,
            filters=filters
        )
        
        # Convert and filter results
        search_results = []
        for result in raw_results:
            search_result = DocumentSearchResult(
                document_id=result.get('document_id'),
                chunk_id=result.get('chunk_id'),
                content=result.get('content', ''),
                score=result.get('score', 0.0),
                section=result.get('section', ''),
                access_level=result.get('access_level', ''),
                chunk_type=result.get('chunk_type'),
                page_number=result.get('page_number'),
                section_name=result.get('section_name'),
                metadata=result.get('metadata', {})
            )
            search_results.append(search_result)
        
        # Sort by score and limit results
        search_results.sort(key=lambda x: x.score, reverse=True)
        return search_results[:limit]
        
    except Exception as e:
        logger.error(f"–ü–æ–∏—Å–∫ –ø–æ —Ä–∞–∑–¥–µ–ª—É –Ω–µ —É–¥–∞–ª—Å—è: {e}")
        return []

async def get_document_info(document_id: int) -> Optional[Dict[str, Any]]:
    """Get basic document information"""
    try:
        # Validate document_id
        if not document_id or document_id <= 0:
            logger.warning(f"‚ö†Ô∏è –ù–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–π document_id: {document_id}")
            return None
            
        db = get_db_session()
        try:
            document = db.query(Document).filter(Document.id == document_id).first()
            if document:
                logger.info(f"üîç get_document_info –¥–ª—è {document_id}:")
                logger.info(f"   - document.title: {document.title}")
                logger.info(f"   - document.original_filename: {document.original_filename}")
                logger.info(f"   - document.filename: {document.filename}")
                
                # –£–º–Ω–∞—è –ª–æ–≥–∏–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è –Ω–∞–∑–≤–∞–Ω–∏—è –¥–æ–∫—É–º–µ–Ω—Ç–∞
                document_title = None
                
                # 1. –ü—Ä–æ–±—É–µ–º title (–µ—Å–ª–∏ –Ω–µ –ø—É—Å—Ç–æ–π –∏ –Ω–µ "string")
                if document.title and document.title != "string" and document.title.strip():
                    document_title = document.title.strip()
                    logger.info(f"   ‚úÖ –ò—Å–ø–æ–ª—å–∑—É–µ–º document.title: '{document_title}'")
                
                # 2. –ï—Å–ª–∏ title –Ω–µ –ø–æ–¥—Ö–æ–¥–∏—Ç, –ø—Ä–æ–±—É–µ–º original_filename
                elif document.original_filename and document.original_filename != "string" and document.original_filename.strip():
                    document_title = document.original_filename.strip()
                    logger.info(f"   ‚úÖ –ò—Å–ø–æ–ª—å–∑—É–µ–º document.original_filename: '{document_title}'")
                
                # 3. –ï—Å–ª–∏ –∏ —ç—Ç–æ –Ω–µ –ø–æ–¥—Ö–æ–¥–∏—Ç, –ø—Ä–æ–±—É–µ–º filename
                elif document.filename and document.filename != "string" and document.filename.strip():
                    document_title = document.filename.strip()
                    logger.info(f"   ‚úÖ –ò—Å–ø–æ–ª—å–∑—É–µ–º document.filename: '{document_title}'")
                
                # 4. Fallback –∫ ID –¥–æ–∫—É–º–µ–Ω—Ç–∞
                else:
                    document_title = f"–î–æ–∫—É–º–µ–Ω—Ç {document_id}"
                    logger.warning(f"   ‚ö†Ô∏è –í—Å–µ –ø–æ–ª—è –ø—É—Å—Ç—ã–µ –∏–ª–∏ 'string', –∏—Å–ø–æ–ª—å–∑—É–µ–º fallback: '{document_title}'")
                
                # –£–±–∏—Ä–∞–µ–º —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ —Ñ–∞–π–ª–∞ –¥–ª—è –ª—É—á—à–µ–≥–æ –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è
                if document_title and '.' in document_title:
                    original_title = document_title
                    document_title = document_title.rsplit('.', 1)[0]
                    logger.info(f"   üîß –£–±—Ä–∞–ª–∏ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ: '{original_title}' ‚Üí '{document_title}'")
                
                logger.info(f"   - –∏—Ç–æ–≥–æ–≤—ã–π document_title: '{document_title}'")
                
                return {
                    "id": document.id,
                    "title": document_title,
                    "section": document.section,
                    "access_level": document.access_level,
                    "filename": document.filename,
                    "original_filename": document.original_filename,
                    "mime_type": document.mime_type,
                    "file_type": document.file_type
                }
            else:
                logger.warning(f"‚ö†Ô∏è –î–æ–∫—É–º–µ–Ω—Ç —Å ID {document_id} –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ –±–∞–∑–µ –¥–∞–Ω–Ω—ã—Ö")
                return None
        finally:
            db.close()
    except Exception as e:
        logger.error(f"‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –¥–æ–∫—É–º–µ–Ω—Ç–µ {document_id}: {e}")
        return None

def get_better_document_name(source_info: Optional[Dict[str, Any]], document_id: int) -> str:
    """Get a better document name for display"""
    logger.info(f"üîç get_better_document_name –¥–ª—è {document_id}:")
    logger.info(f"   - source_info: {source_info}")
    
    if source_info and source_info.get("title"):
        title = source_info["title"]
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ title –Ω–µ —Ä–∞–≤–µ–Ω "string" –∏ –Ω–µ –ø—É—Å—Ç–æ–π
        if not title or title == "string" or title.strip() == "":
            logger.error(f"‚ùå title —Ä–∞–≤–µ–Ω 'string' –∏–ª–∏ –ø—É—Å—Ç–æ–π –¥–ª—è –¥–æ–∫—É–º–µ–Ω—Ç–∞ {document_id}")
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º fallback
            fallback_name = f"–î–æ–∫—É–º–µ–Ω—Ç {document_id}"
            logger.info(f"   - –∏—Å–ø–æ–ª—å–∑—É–µ–º fallback –∏–∑-–∑–∞ 'string' –∏–ª–∏ –ø—É—Å—Ç–æ–≥–æ: '{fallback_name}'")
            return fallback_name
        
        # –£–±–∏—Ä–∞–µ–º —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ —Ñ–∞–π–ª–∞ –µ—Å–ª–∏ –µ—Å—Ç—å
        if '.' in title:
            original_title = title
            title = title.rsplit('.', 1)[0]
            logger.info(f"   üîß –£–±—Ä–∞–ª–∏ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ: '{original_title}' ‚Üí '{title}'")
        
        logger.info(f"   - –∏—Å–ø–æ–ª—å–∑—É–µ–º title: '{title}'")
        return title
    
    # Fallback –∫ –±–æ–ª–µ–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ç–∏–≤–Ω–æ–º—É –Ω–∞–∑–≤–∞–Ω–∏—é
    fallback_name = f"–î–æ–∫—É–º–µ–Ω—Ç {document_id}"
    logger.info(f"   - –∏—Å–ø–æ–ª—å–∑—É–µ–º fallback: '{fallback_name}'")
    return fallback_name


@app.exception_handler(Exception)
async def global_exception_handler(request, exc):
    """Global exception handler"""
    logger.error(f"–ù–µ–æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω–∞—è –æ—à–∏–±–∫–∞: {exc}")
    return JSONResponse(
        status_code=500,
        content={"detail": "–í–Ω—É—Ç—Ä–µ–Ω–Ω—è—è –æ—à–∏–±–∫–∞ —Å–µ—Ä–≤–µ—Ä–∞"}
    )

async def _filter_relevant_sources(sources: List[Dict[str, Any]], ai_response: str, context_chunks: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
    """
    –£–º–Ω–∞—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ - –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç —Ç–æ–ª—å–∫–æ —Ç–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã, 
    –∫–æ—Ç–æ—Ä—ã–µ –¥–µ–π—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ —Å–æ–¥–µ—Ä–∂–∞—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é, –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–Ω—É—é –≤ –æ—Ç–≤–µ—Ç–µ
    """
    try:
        if not sources or not context_chunks:
            logger.info("üîç –ù–µ—Ç –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ –∏–ª–∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –¥–ª—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏")
            return sources
        
        logger.info(f"üîç –§–∏–ª—å—Ç—Ä—É–µ–º {len(sources)} –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ –ø–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏ –∫ –æ—Ç–≤–µ—Ç—É")
        
        # –°–æ–∑–¥–∞–µ–º —Å–ª–æ–≤–∞—Ä—å –¥–ª—è –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
        document_usage = {}
        
        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –∫–∞–∂–¥—ã–π —á–∞–Ω–∫ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
        for chunk in context_chunks:
            doc_id = chunk.get('document_id')
            if doc_id:
                if doc_id not in document_usage:
                    document_usage[doc_id] = {
                        'chunks': [],
                        'total_score': 0,
                        'max_score': 0,
                        'content_length': 0
                    }
                
                # –î–æ–±–∞–≤–ª—è–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —á–∞–Ω–∫–µ
                document_usage[doc_id]['chunks'].append(chunk)
                document_usage[doc_id]['total_score'] += chunk.get('score', 0)
                document_usage[doc_id]['max_score'] = max(document_usage[doc_id]['max_score'], chunk.get('score', 0))
                document_usage[doc_id]['content_length'] += len(chunk.get('content', ''))
        
        # –§–∏–ª—å—Ç—Ä—É–µ–º –∏—Å—Ç–æ—á–Ω–∏–∫–∏ –ø–æ –∫—Ä–∏—Ç–µ—Ä–∏—è–º —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏
        filtered_sources = []
        for source in sources:
            doc_id = source.get('document_id')
            if doc_id in document_usage:
                usage_info = document_usage[doc_id]
                
                # –ö—Ä–∏—Ç–µ—Ä–∏–∏ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏ (—É–∂–µ—Å—Ç–æ—á–µ–Ω–Ω—ã–µ):
                # 1. –í—ã—Å–æ–∫–∏–π –º–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π score (> 0.7) –ò–õ–ò
                # 2. –°—Ä–µ–¥–Ω–∏–π score (> 0.5) –ò –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞ (> 200 —Å–∏–º–≤–æ–ª–æ–≤) –ò –Ω–µ—Å–∫–æ–ª—å–∫–æ —á–∞–Ω–∫–æ–≤ (> 1)
                is_relevant = (
                    usage_info['max_score'] > 0.7 or  # –í—ã—Å–æ–∫–∞—è —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å
                    (usage_info['max_score'] > 0.5 and  # –°—Ä–µ–¥–Ω—è—è —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å
                     usage_info['content_length'] > 200 and  # –î–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞
                     len(usage_info['chunks']) > 1)  # –ù–µ—Å–∫–æ–ª—å–∫–æ —á–∞–Ω–∫–æ–≤
                )
                
                if is_relevant:
                    # –î–æ–±–∞–≤–ª—è–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ–± –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–∏
                    source['usage_info'] = {
                        'chunks_used': len(usage_info['chunks']),
                        'max_score': usage_info['max_score'],
                        'total_content_length': usage_info['content_length']
                    }
                    filtered_sources.append(source)
                    logger.info(f"‚úÖ –î–æ–∫—É–º–µ–Ω—Ç {doc_id} –≤–∫–ª—é—á–µ–Ω (score: {usage_info['max_score']:.2f}, —á–∞–Ω–∫–æ–≤: {len(usage_info['chunks'])})")
                else:
                    logger.info(f"‚ùå –î–æ–∫—É–º–µ–Ω—Ç {doc_id} –∏—Å–∫–ª—é—á–µ–Ω (score: {usage_info['max_score']:.2f}, —á–∞–Ω–∫–æ–≤: {len(usage_info['chunks'])})")
            else:
                # –ï—Å–ª–∏ –¥–æ–∫—É–º–µ–Ω—Ç –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ, –∏—Å–∫–ª—é—á–∞–µ–º –µ–≥–æ
                logger.warning(f"‚ö†Ô∏è –î–æ–∫—É–º–µ–Ω—Ç {doc_id} –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ, –∏—Å–∫–ª—é—á–∞–µ–º")
        
        logger.info(f"üéØ –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞: {len(filtered_sources)} –∏–∑ {len(sources)} –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã")
        
        # üîÑ FALLBACK: –µ—Å–ª–∏ –≤—Å–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏ –æ—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω—ã, –Ω–æ –µ—Å—Ç—å –∫–æ–Ω—Ç–µ–∫—Å—Ç
        # –ø–æ–∫–∞–∑—ã–≤–∞–µ–º —Ö–æ—Ç—è –±—ã –æ–¥–∏–Ω –Ω–∞–∏–±–æ–ª–µ–µ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–π
        if not filtered_sources and sources and context_chunks:
            logger.info(f"‚ö†Ô∏è –í—Å–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏ –æ—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω—ã, –∏—Å–ø–æ–ª—å–∑—É–µ–º fallback –ª–æ–≥–∏–∫—É")
            
            # –ù–∞—Ö–æ–¥–∏–º –∏—Å—Ç–æ—á–Ω–∏–∫ —Å –Ω–∞–∏–≤—ã—Å—à–∏–º score
            best_source = max(sources, key=lambda s: document_usage.get(s.get('document_id', 0), {}).get('max_score', 0))
            best_doc_id = best_source.get('document_id')
            
            if best_doc_id and best_doc_id in document_usage:
                best_score = document_usage[best_doc_id]['max_score']
                logger.info(f"‚úÖ Fallback: –≤–∫–ª—é—á–∞–µ–º –¥–æ–∫—É–º–µ–Ω—Ç {best_doc_id} —Å score {best_score:.2f}")
                
                # –î–æ–±–∞–≤–ª—è–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ–± –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–∏
                best_source['usage_info'] = {
                    'chunks_used': len(document_usage[best_doc_id]['chunks']),
                    'max_score': best_score,
                    'total_content_length': document_usage[best_doc_id]['content_length'],
                    'fallback_included': True
                }
                filtered_sources = [best_source]
        
        return filtered_sources
        
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤: {e}")
        # –í —Å–ª—É—á–∞–µ –æ—à–∏–±–∫–∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –≤—Å–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏
        return sources

if __name__ == "__main__":
    uvicorn.run(
        "main:app",
        host="0.0.0.0",
        port=8000,
        reload=settings.debug,
        log_level=settings.log_level.lower()
    )
